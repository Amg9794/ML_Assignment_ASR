{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "944c9a22",
   "metadata": {},
   "source": [
    "I will use [NeMo Forced Aligner](https://github.com/NVIDIA/NeMo/tree/main/tools/nemo_forced_aligner) to generate token and word alignments for a given audio. \n",
    "\n",
    "The alignment process is shown below\n",
    "\n",
    "![NFA forced alignment pipeline](https://github.com/NVIDIA/NeMo/releases/download/v1.20.0/nfa_forced_alignment_pipeline.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f056c2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "BRANCH=\"main\"\n",
    "import os\n",
    "import sys\n",
    "\n",
    "NEMO_DIR_PATH = \"/hdd2/Aman/scripts/ML_assign/src/Nemo\"\n",
    "\n",
    "# # Option 2: Clone and install NeMo since it doesn't exist\n",
    "# if not os.path.exists(NEMO_DIR_PATH):\n",
    "#     !git clone -b {BRANCH} https://github.com/NVIDIA/NeMo\n",
    "#     %cd NeMo\n",
    "#     !python3 -m pip install git+https://github.com/NVIDIA/NeMo.git@{BRANCH}#egg=nemo_toolkit[all]\n",
    "#     %cd -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9eea75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running alignment for: /hdd2/Aman/scripts/ML_assign/Dataset/manifest/case2_2023_11_23_manifest.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-05-07 17:48:37 align:159] Hydra config: pretrained_name: stt_en_fastconformer_hybrid_large_pc\n",
      "    model_path: null\n",
      "    manifest_filepath: /hdd2/Aman/scripts/ML_assign/Dataset/manifest/case2_2023_11_23_manifest.json\n",
      "    output_dir: /hdd2/Aman/scripts/ML_assign/src/nfa_output/case2_2023_11_23_manifest\n",
      "    align_using_pred_text: false\n",
      "    transcribe_device: cpu\n",
      "    viterbi_device: cpu\n",
      "    batch_size: 1\n",
      "    use_local_attention: true\n",
      "    additional_segment_grouping_separator: '|'\n",
      "    audio_filepath_parts_in_utt_id: 1\n",
      "    use_buffered_chunked_streaming: false\n",
      "    chunk_len_in_secs: 1.6\n",
      "    total_buffer_in_secs: 4.0\n",
      "    chunk_batch_size: 32\n",
      "    simulate_cache_aware_streaming: false\n",
      "    save_output_file_formats:\n",
      "    - ctm\n",
      "    - ass\n",
      "    ctm_file_config:\n",
      "      remove_blank_tokens: false\n",
      "      minimum_timestamp_duration: 0.0\n",
      "    ass_file_config:\n",
      "      fontsize: 20\n",
      "      vertical_alignment: center\n",
      "      resegment_text_to_fill_space: false\n",
      "      max_lines_per_segment: 2\n",
      "      text_already_spoken_rgb:\n",
      "      - 49\n",
      "      - 46\n",
      "      - 61\n",
      "      text_being_spoken_rgb:\n",
      "      - 57\n",
      "      - 171\n",
      "      - 9\n",
      "      text_not_yet_spoken_rgb:\n",
      "      - 194\n",
      "      - 193\n",
      "      - 199\n",
      "    \n",
      "[NeMo I 2025-05-07 17:48:37 align:228] Device to be used for transcription step (`transcribe_device`) is cpu\n",
      "[NeMo I 2025-05-07 17:48:37 align:234] Device to be used for viterbi step (`viterbi_device`) is cpu\n",
      "[NeMo I 2025-05-07 17:48:37 cloud:58] Found existing object /home/antpc/.cache/torch/NeMo/NeMo_2.4.0rc0/stt_en_fastconformer_hybrid_large_pc/465b32000fc320f5905fda11a1866ef6/stt_en_fastconformer_hybrid_large_pc.nemo.\n",
      "[NeMo I 2025-05-07 17:48:37 cloud:64] Re-using file from: /home/antpc/.cache/torch/NeMo/NeMo_2.4.0rc0/stt_en_fastconformer_hybrid_large_pc/465b32000fc320f5905fda11a1866ef6/stt_en_fastconformer_hybrid_large_pc.nemo\n",
      "[NeMo I 2025-05-07 17:48:37 common:827] Instantiating model from pre-trained checkpoint\n",
      "[NeMo I 2025-05-07 17:48:38 mixins:181] Tokenizer SentencePieceTokenizer initialized with 1024 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2025-05-07 17:48:39 modelPT:180] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
      "    Train config : \n",
      "    is_concat: false\n",
      "    concat_sampling_technique: random\n",
      "    concat_sampling_temperature: 8\n",
      "    concat_shuffle: true\n",
      "    concat_sampling_scale: 1.0\n",
      "    manifest_filepath:\n",
      "    - - /data/mml/en/tarred_train/pcstrip_sharded_manifests/manifest__OP_0..511_CL_.json\n",
      "    tarred_audio_filepaths:\n",
      "    - - /data/mml/en/tarred_train/audio__OP_0..511_CL_.tar\n",
      "    concat_sampling_probabilities:\n",
      "    - 1.0\n",
      "    sample_rate: 16000\n",
      "    batch_size: 16\n",
      "    shuffle: true\n",
      "    num_workers: 8\n",
      "    pin_memory: true\n",
      "    use_start_end_token: false\n",
      "    trim_silence: false\n",
      "    max_duration: 20\n",
      "    min_duration: 0.1\n",
      "    is_tarred: true\n",
      "    shard_manifests: true\n",
      "    defer_setup: true\n",
      "    shuffle_n: 528387\n",
      "    bucketing_strategy: synced_randomized\n",
      "    bucketing_batch_size: null\n",
      "    \n",
      "[NeMo W 2025-05-07 17:48:39 modelPT:187] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
      "    Validation config : \n",
      "    ds_item:\n",
      "    - name: en_\n",
      "      is_concat: true\n",
      "      concat_sampling_technique: random\n",
      "      concat_sampling_seed: 1234\n",
      "      concat_shuffle: false\n",
      "      concat_sampling_scale: 0.25\n",
      "      concat_sampling_probabilities:\n",
      "      - 0.088929287\n",
      "      - 0.116404906\n",
      "      - 0.101879436\n",
      "      - 0.171855007\n",
      "      - 0.160962192\n",
      "      - 0.092674082\n",
      "      - 0.041152255\n",
      "      - 0.226142835\n",
      "      manifest_filepath:\n",
      "      - - /data/mml/en/val_test/fisher/audio_manifest_dev_clean_pcstrip_en.json\n",
      "        - /data/mml/en/val_test/europarl/audio_manifest_dev_clean_pcstrip_en.json\n",
      "        - /data/mml/en/val_test/librispeech/audio_manifest_dev_clean_pcstrip_en.json\n",
      "        - /data/mml/en/val_test/mcv11/audio_manifest_dev_clean_pcstrip_en.json\n",
      "        - /data/mml/en/val_test/nsc1/audio_manifest_dev_clean_pcstrip_en.json\n",
      "        - /data/mml/en/val_test/SPGI/audio_manifest_dev_clean_pcstrip_en.json\n",
      "        - /data/mml/en/val_test/voxpopuli/audio_manifest_dev_clean_pcstrip_en.json\n",
      "        - /data/mml/en/val_test/mls_dev/mls_dev_pcstrip_en.json\n",
      "      sample_rate: 16000\n",
      "      batch_size: 64\n",
      "      shuffle: false\n",
      "      num_workers: 1\n",
      "      pin_memory: true\n",
      "      max_duration: 20.0\n",
      "      min_duration: 0.1\n",
      "      use_start_end_token: false\n",
      "    - name: fisher_\n",
      "      is_concat: false\n",
      "      manifest_filepath: /data/mml/en/val_test/fisher/audio_manifest_dev_clean_pcstrip_en.json\n",
      "      sample_rate: 16000\n",
      "      batch_size: 64\n",
      "      shuffle: false\n",
      "      num_workers: 1\n",
      "      pin_memory: true\n",
      "      max_duration: 20.0\n",
      "      min_duration: 0.1\n",
      "      use_start_end_token: false\n",
      "    - name: europarl_\n",
      "      is_concat: false\n",
      "      manifest_filepath: /data/mml/en/val_test/europarl/audio_manifest_dev_clean_pcstrip_en.json\n",
      "      sample_rate: 16000\n",
      "      batch_size: 64\n",
      "      shuffle: false\n",
      "      num_workers: 1\n",
      "      pin_memory: true\n",
      "      max_duration: 20.0\n",
      "      min_duration: 0.1\n",
      "      use_start_end_token: false\n",
      "    - name: librispeech_\n",
      "      is_concat: false\n",
      "      manifest_filepath: /data/mml/en/val_test/librispeech/audio_manifest_dev_clean_pcstrip_en.json\n",
      "      sample_rate: 16000\n",
      "      batch_size: 64\n",
      "      shuffle: false\n",
      "      num_workers: 1\n",
      "      pin_memory: true\n",
      "      max_duration: 20.0\n",
      "      min_duration: 0.1\n",
      "      use_start_end_token: false\n",
      "    - name: mcv11_\n",
      "      is_concat: false\n",
      "      manifest_filepath: /data/mml/en/val_test/mcv11/audio_manifest_dev_clean_pcstrip_en.json\n",
      "      sample_rate: 16000\n",
      "      batch_size: 64\n",
      "      shuffle: false\n",
      "      num_workers: 1\n",
      "      pin_memory: true\n",
      "      max_duration: 20.0\n",
      "      min_duration: 0.1\n",
      "      use_start_end_token: false\n",
      "    - name: nsc1_\n",
      "      is_concat: false\n",
      "      manifest_filepath: /data/mml/en/val_test/nsc1/audio_manifest_dev_clean_pcstrip_en.json\n",
      "      sample_rate: 16000\n",
      "      batch_size: 64\n",
      "      shuffle: false\n",
      "      num_workers: 1\n",
      "      pin_memory: true\n",
      "      max_duration: 20.0\n",
      "      min_duration: 0.1\n",
      "      use_start_end_token: false\n",
      "    - name: spgi_\n",
      "      is_concat: false\n",
      "      manifest_filepath: /data/mml/en/val_test/SPGI/audio_manifest_dev_clean_pcstrip_en.json\n",
      "      sample_rate: 16000\n",
      "      batch_size: 64\n",
      "      shuffle: false\n",
      "      num_workers: 1\n",
      "      pin_memory: true\n",
      "      max_duration: 20.0\n",
      "      min_duration: 0.1\n",
      "      use_start_end_token: false\n",
      "    - name: voxpopuli_\n",
      "      is_concat: false\n",
      "      manifest_filepath: /data/mml/en/val_test/voxpopuli/audio_manifest_dev_clean_pcstrip_en.json\n",
      "      sample_rate: 16000\n",
      "      batch_size: 64\n",
      "      shuffle: false\n",
      "      num_workers: 1\n",
      "      pin_memory: true\n",
      "      max_duration: 20.0\n",
      "      min_duration: 0.1\n",
      "      use_start_end_token: false\n",
      "    - name: mls_\n",
      "      is_concat: false\n",
      "      manifest_filepath: /data/mml/en/val_test/mls_dev/mls_dev_pcstrip_en.json\n",
      "      sample_rate: 16000\n",
      "      batch_size: 64\n",
      "      shuffle: false\n",
      "      num_workers: 1\n",
      "      pin_memory: true\n",
      "      max_duration: 20.0\n",
      "      min_duration: 0.1\n",
      "      use_start_end_token: false\n",
      "    max_duration: 20\n",
      "    \n",
      "[NeMo W 2025-05-07 17:48:39 modelPT:194] Please call the ModelPT.setup_test_data() or ModelPT.setup_multiple_test_data() method and provide a valid configuration file to setup the test data loader(s).\n",
      "    Test config : \n",
      "    manifest_filepath: null\n",
      "    sample_rate: 16000\n",
      "    batch_size: 16\n",
      "    shuffle: false\n",
      "    num_workers: 8\n",
      "    pin_memory: true\n",
      "    use_start_end_token: false\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-05-07 17:48:40 features:305] PADDING: 0\n",
      "[NeMo I 2025-05-07 17:48:41 rnnt_models:226] Using RNNT Loss : warprnnt_numba\n",
      "    Loss warprnnt_numba_kwargs: {'fastemit_lambda': 0.0, 'clamp': -1.0}\n",
      "[NeMo I 2025-05-07 17:48:41 rnnt_models:226] Using RNNT Loss : warprnnt_numba\n",
      "    Loss warprnnt_numba_kwargs: {'fastemit_lambda': 0.0, 'clamp': -1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2025-05-07 17:48:41 rnnt_loop_labels_computer:290] No conditional node support for Cuda.\n",
      "    Cuda graphs with while loops are disabled, decoding speed will be slower\n",
      "    Reason: Driver supports cuda toolkit version 12.2, but the driver needs to support at least 12,3. Please update your cuda driver.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-05-07 17:48:41 rnnt_models:226] Using RNNT Loss : warprnnt_numba\n",
      "    Loss warprnnt_numba_kwargs: {'fastemit_lambda': 0.0, 'clamp': -1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2025-05-07 17:48:41 rnnt_loop_labels_computer:290] No conditional node support for Cuda.\n",
      "    Cuda graphs with while loops are disabled, decoding speed will be slower\n",
      "    Reason: Driver supports cuda toolkit version 12.2, but the driver needs to support at least 12,3. Please update your cuda driver.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-05-07 17:48:42 save_restore_connector:275] Model EncDecHybridRNNTCTCBPEModel was successfully restored from /home/antpc/.cache/torch/NeMo/NeMo_2.4.0rc0/stt_en_fastconformer_hybrid_large_pc/465b32000fc320f5905fda11a1866ef6/stt_en_fastconformer_hybrid_large_pc.nemo.\n",
      "[NeMo I 2025-05-07 17:48:42 hybrid_rnnt_ctc_bpe_models:487] No `decoding_cfg` passed when changing decoding strategy, using internal config\n",
      "[NeMo I 2025-05-07 17:48:42 hybrid_rnnt_ctc_bpe_models:512] Changed decoding strategy of the CTC decoder to \n",
      "    strategy: greedy\n",
      "    preserve_alignments: null\n",
      "    compute_timestamps: null\n",
      "    word_seperator: ' '\n",
      "    segment_seperators:\n",
      "    - .\n",
      "    - '!'\n",
      "    - '?'\n",
      "    segment_gap_threshold: null\n",
      "    ctc_timestamp_type: all\n",
      "    batch_dim_index: 0\n",
      "    greedy:\n",
      "      preserve_alignments: false\n",
      "      compute_timestamps: false\n",
      "      preserve_frame_confidence: false\n",
      "      confidence_method_cfg:\n",
      "        name: entropy\n",
      "        entropy_type: tsallis\n",
      "        alpha: 0.33\n",
      "        entropy_norm: exp\n",
      "        temperature: DEPRECATED\n",
      "    beam:\n",
      "      beam_size: 4\n",
      "      search_type: default\n",
      "      preserve_alignments: false\n",
      "      compute_timestamps: false\n",
      "      return_best_hypothesis: true\n",
      "      beam_alpha: 1.0\n",
      "      beam_beta: 0.0\n",
      "      kenlm_path: null\n",
      "      flashlight_cfg:\n",
      "        lexicon_path: null\n",
      "        boost_path: null\n",
      "        beam_size_token: 16\n",
      "        beam_threshold: 20.0\n",
      "        unk_weight: -.inf\n",
      "        sil_weight: 0.0\n",
      "      pyctcdecode_cfg:\n",
      "        beam_prune_logp: -10.0\n",
      "        token_min_logp: -5.0\n",
      "        prune_history: false\n",
      "        hotwords: null\n",
      "        hotword_weight: 10.0\n",
      "    wfst:\n",
      "      beam_size: 4\n",
      "      search_type: riva\n",
      "      return_best_hypothesis: true\n",
      "      preserve_alignments: false\n",
      "      compute_timestamps: false\n",
      "      decoding_mode: nbest\n",
      "      open_vocabulary_decoding: false\n",
      "      beam_width: 10.0\n",
      "      lm_weight: 1.0\n",
      "      device: cuda\n",
      "      arpa_lm_path: null\n",
      "      wfst_lm_path: null\n",
      "      riva_decoding_cfg: {}\n",
      "      k2_decoding_cfg:\n",
      "        search_beam: 20.0\n",
      "        output_beam: 10.0\n",
      "        min_active_states: 30\n",
      "        max_active_states: 10000\n",
      "    confidence_cfg:\n",
      "      preserve_frame_confidence: false\n",
      "      preserve_token_confidence: false\n",
      "      preserve_word_confidence: false\n",
      "      exclude_blank: true\n",
      "      aggregation: min\n",
      "      tdt_include_duration: false\n",
      "      method_cfg:\n",
      "        name: entropy\n",
      "        entropy_type: tsallis\n",
      "        alpha: 0.33\n",
      "        entropy_norm: exp\n",
      "        temperature: DEPRECATED\n",
      "    temperature: 1.0\n",
      "    \n",
      "[NeMo I 2025-05-07 17:48:42 align:250] Flag use_local_attention is set to True => will try to use local attention for model if it allows it\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Transcribing:   0%|          | 0/1 [00:00<?, ?it/s][NeMo W 2025-05-07 17:48:47 ctc_greedy_decoding:168] CTC decoding strategy 'greedy' is slower than 'greedy_batch', which implements the same exact interface. Consider changing your strategy to 'greedy_batch' for a free performance improvement.\n",
      "Transcribing: 100%|██████████| 1/1 [00:05<00:00,  5.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-05-07 17:48:47 data_prep:828] Calculated that the model downsample factor is 8 and therefore the ASR model output timestep duration is 0.08 -- will use this for all batches\n",
      "Running alignment for: /hdd2/Aman/scripts/ML_assign/Dataset/manifest/case14_2024_09_01_manifest.json\n",
      "[NeMo I 2025-05-07 17:49:02 align:159] Hydra config: pretrained_name: stt_en_fastconformer_hybrid_large_pc\n",
      "    model_path: null\n",
      "    manifest_filepath: /hdd2/Aman/scripts/ML_assign/Dataset/manifest/case14_2024_09_01_manifest.json\n",
      "    output_dir: /hdd2/Aman/scripts/ML_assign/src/nfa_output/case14_2024_09_01_manifest\n",
      "    align_using_pred_text: false\n",
      "    transcribe_device: cpu\n",
      "    viterbi_device: cpu\n",
      "    batch_size: 1\n",
      "    use_local_attention: true\n",
      "    additional_segment_grouping_separator: '|'\n",
      "    audio_filepath_parts_in_utt_id: 1\n",
      "    use_buffered_chunked_streaming: false\n",
      "    chunk_len_in_secs: 1.6\n",
      "    total_buffer_in_secs: 4.0\n",
      "    chunk_batch_size: 32\n",
      "    simulate_cache_aware_streaming: false\n",
      "    save_output_file_formats:\n",
      "    - ctm\n",
      "    - ass\n",
      "    ctm_file_config:\n",
      "      remove_blank_tokens: false\n",
      "      minimum_timestamp_duration: 0.0\n",
      "    ass_file_config:\n",
      "      fontsize: 20\n",
      "      vertical_alignment: center\n",
      "      resegment_text_to_fill_space: false\n",
      "      max_lines_per_segment: 2\n",
      "      text_already_spoken_rgb:\n",
      "      - 49\n",
      "      - 46\n",
      "      - 61\n",
      "      text_being_spoken_rgb:\n",
      "      - 57\n",
      "      - 171\n",
      "      - 9\n",
      "      text_not_yet_spoken_rgb:\n",
      "      - 194\n",
      "      - 193\n",
      "      - 199\n",
      "    \n",
      "[NeMo I 2025-05-07 17:49:02 align:228] Device to be used for transcription step (`transcribe_device`) is cpu\n",
      "[NeMo I 2025-05-07 17:49:02 align:234] Device to be used for viterbi step (`viterbi_device`) is cpu\n",
      "[NeMo I 2025-05-07 17:49:02 cloud:58] Found existing object /home/antpc/.cache/torch/NeMo/NeMo_2.4.0rc0/stt_en_fastconformer_hybrid_large_pc/465b32000fc320f5905fda11a1866ef6/stt_en_fastconformer_hybrid_large_pc.nemo.\n",
      "[NeMo I 2025-05-07 17:49:02 cloud:64] Re-using file from: /home/antpc/.cache/torch/NeMo/NeMo_2.4.0rc0/stt_en_fastconformer_hybrid_large_pc/465b32000fc320f5905fda11a1866ef6/stt_en_fastconformer_hybrid_large_pc.nemo\n",
      "[NeMo I 2025-05-07 17:49:02 common:827] Instantiating model from pre-trained checkpoint\n",
      "[NeMo I 2025-05-07 17:49:04 mixins:181] Tokenizer SentencePieceTokenizer initialized with 1024 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2025-05-07 17:49:05 modelPT:180] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
      "    Train config : \n",
      "    is_concat: false\n",
      "    concat_sampling_technique: random\n",
      "    concat_sampling_temperature: 8\n",
      "    concat_shuffle: true\n",
      "    concat_sampling_scale: 1.0\n",
      "    manifest_filepath:\n",
      "    - - /data/mml/en/tarred_train/pcstrip_sharded_manifests/manifest__OP_0..511_CL_.json\n",
      "    tarred_audio_filepaths:\n",
      "    - - /data/mml/en/tarred_train/audio__OP_0..511_CL_.tar\n",
      "    concat_sampling_probabilities:\n",
      "    - 1.0\n",
      "    sample_rate: 16000\n",
      "    batch_size: 16\n",
      "    shuffle: true\n",
      "    num_workers: 8\n",
      "    pin_memory: true\n",
      "    use_start_end_token: false\n",
      "    trim_silence: false\n",
      "    max_duration: 20\n",
      "    min_duration: 0.1\n",
      "    is_tarred: true\n",
      "    shard_manifests: true\n",
      "    defer_setup: true\n",
      "    shuffle_n: 528387\n",
      "    bucketing_strategy: synced_randomized\n",
      "    bucketing_batch_size: null\n",
      "    \n",
      "[NeMo W 2025-05-07 17:49:05 modelPT:187] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
      "    Validation config : \n",
      "    ds_item:\n",
      "    - name: en_\n",
      "      is_concat: true\n",
      "      concat_sampling_technique: random\n",
      "      concat_sampling_seed: 1234\n",
      "      concat_shuffle: false\n",
      "      concat_sampling_scale: 0.25\n",
      "      concat_sampling_probabilities:\n",
      "      - 0.088929287\n",
      "      - 0.116404906\n",
      "      - 0.101879436\n",
      "      - 0.171855007\n",
      "      - 0.160962192\n",
      "      - 0.092674082\n",
      "      - 0.041152255\n",
      "      - 0.226142835\n",
      "      manifest_filepath:\n",
      "      - - /data/mml/en/val_test/fisher/audio_manifest_dev_clean_pcstrip_en.json\n",
      "        - /data/mml/en/val_test/europarl/audio_manifest_dev_clean_pcstrip_en.json\n",
      "        - /data/mml/en/val_test/librispeech/audio_manifest_dev_clean_pcstrip_en.json\n",
      "        - /data/mml/en/val_test/mcv11/audio_manifest_dev_clean_pcstrip_en.json\n",
      "        - /data/mml/en/val_test/nsc1/audio_manifest_dev_clean_pcstrip_en.json\n",
      "        - /data/mml/en/val_test/SPGI/audio_manifest_dev_clean_pcstrip_en.json\n",
      "        - /data/mml/en/val_test/voxpopuli/audio_manifest_dev_clean_pcstrip_en.json\n",
      "        - /data/mml/en/val_test/mls_dev/mls_dev_pcstrip_en.json\n",
      "      sample_rate: 16000\n",
      "      batch_size: 64\n",
      "      shuffle: false\n",
      "      num_workers: 1\n",
      "      pin_memory: true\n",
      "      max_duration: 20.0\n",
      "      min_duration: 0.1\n",
      "      use_start_end_token: false\n",
      "    - name: fisher_\n",
      "      is_concat: false\n",
      "      manifest_filepath: /data/mml/en/val_test/fisher/audio_manifest_dev_clean_pcstrip_en.json\n",
      "      sample_rate: 16000\n",
      "      batch_size: 64\n",
      "      shuffle: false\n",
      "      num_workers: 1\n",
      "      pin_memory: true\n",
      "      max_duration: 20.0\n",
      "      min_duration: 0.1\n",
      "      use_start_end_token: false\n",
      "    - name: europarl_\n",
      "      is_concat: false\n",
      "      manifest_filepath: /data/mml/en/val_test/europarl/audio_manifest_dev_clean_pcstrip_en.json\n",
      "      sample_rate: 16000\n",
      "      batch_size: 64\n",
      "      shuffle: false\n",
      "      num_workers: 1\n",
      "      pin_memory: true\n",
      "      max_duration: 20.0\n",
      "      min_duration: 0.1\n",
      "      use_start_end_token: false\n",
      "    - name: librispeech_\n",
      "      is_concat: false\n",
      "      manifest_filepath: /data/mml/en/val_test/librispeech/audio_manifest_dev_clean_pcstrip_en.json\n",
      "      sample_rate: 16000\n",
      "      batch_size: 64\n",
      "      shuffle: false\n",
      "      num_workers: 1\n",
      "      pin_memory: true\n",
      "      max_duration: 20.0\n",
      "      min_duration: 0.1\n",
      "      use_start_end_token: false\n",
      "    - name: mcv11_\n",
      "      is_concat: false\n",
      "      manifest_filepath: /data/mml/en/val_test/mcv11/audio_manifest_dev_clean_pcstrip_en.json\n",
      "      sample_rate: 16000\n",
      "      batch_size: 64\n",
      "      shuffle: false\n",
      "      num_workers: 1\n",
      "      pin_memory: true\n",
      "      max_duration: 20.0\n",
      "      min_duration: 0.1\n",
      "      use_start_end_token: false\n",
      "    - name: nsc1_\n",
      "      is_concat: false\n",
      "      manifest_filepath: /data/mml/en/val_test/nsc1/audio_manifest_dev_clean_pcstrip_en.json\n",
      "      sample_rate: 16000\n",
      "      batch_size: 64\n",
      "      shuffle: false\n",
      "      num_workers: 1\n",
      "      pin_memory: true\n",
      "      max_duration: 20.0\n",
      "      min_duration: 0.1\n",
      "      use_start_end_token: false\n",
      "    - name: spgi_\n",
      "      is_concat: false\n",
      "      manifest_filepath: /data/mml/en/val_test/SPGI/audio_manifest_dev_clean_pcstrip_en.json\n",
      "      sample_rate: 16000\n",
      "      batch_size: 64\n",
      "      shuffle: false\n",
      "      num_workers: 1\n",
      "      pin_memory: true\n",
      "      max_duration: 20.0\n",
      "      min_duration: 0.1\n",
      "      use_start_end_token: false\n",
      "    - name: voxpopuli_\n",
      "      is_concat: false\n",
      "      manifest_filepath: /data/mml/en/val_test/voxpopuli/audio_manifest_dev_clean_pcstrip_en.json\n",
      "      sample_rate: 16000\n",
      "      batch_size: 64\n",
      "      shuffle: false\n",
      "      num_workers: 1\n",
      "      pin_memory: true\n",
      "      max_duration: 20.0\n",
      "      min_duration: 0.1\n",
      "      use_start_end_token: false\n",
      "    - name: mls_\n",
      "      is_concat: false\n",
      "      manifest_filepath: /data/mml/en/val_test/mls_dev/mls_dev_pcstrip_en.json\n",
      "      sample_rate: 16000\n",
      "      batch_size: 64\n",
      "      shuffle: false\n",
      "      num_workers: 1\n",
      "      pin_memory: true\n",
      "      max_duration: 20.0\n",
      "      min_duration: 0.1\n",
      "      use_start_end_token: false\n",
      "    max_duration: 20\n",
      "    \n",
      "[NeMo W 2025-05-07 17:49:05 modelPT:194] Please call the ModelPT.setup_test_data() or ModelPT.setup_multiple_test_data() method and provide a valid configuration file to setup the test data loader(s).\n",
      "    Test config : \n",
      "    manifest_filepath: null\n",
      "    sample_rate: 16000\n",
      "    batch_size: 16\n",
      "    shuffle: false\n",
      "    num_workers: 8\n",
      "    pin_memory: true\n",
      "    use_start_end_token: false\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-05-07 17:49:05 features:305] PADDING: 0\n",
      "[NeMo I 2025-05-07 17:49:06 rnnt_models:226] Using RNNT Loss : warprnnt_numba\n",
      "    Loss warprnnt_numba_kwargs: {'fastemit_lambda': 0.0, 'clamp': -1.0}\n",
      "[NeMo I 2025-05-07 17:49:06 rnnt_models:226] Using RNNT Loss : warprnnt_numba\n",
      "    Loss warprnnt_numba_kwargs: {'fastemit_lambda': 0.0, 'clamp': -1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2025-05-07 17:49:07 rnnt_loop_labels_computer:290] No conditional node support for Cuda.\n",
      "    Cuda graphs with while loops are disabled, decoding speed will be slower\n",
      "    Reason: Driver supports cuda toolkit version 12.2, but the driver needs to support at least 12,3. Please update your cuda driver.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-05-07 17:49:07 rnnt_models:226] Using RNNT Loss : warprnnt_numba\n",
      "    Loss warprnnt_numba_kwargs: {'fastemit_lambda': 0.0, 'clamp': -1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2025-05-07 17:49:07 rnnt_loop_labels_computer:290] No conditional node support for Cuda.\n",
      "    Cuda graphs with while loops are disabled, decoding speed will be slower\n",
      "    Reason: Driver supports cuda toolkit version 12.2, but the driver needs to support at least 12,3. Please update your cuda driver.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-05-07 17:49:07 save_restore_connector:275] Model EncDecHybridRNNTCTCBPEModel was successfully restored from /home/antpc/.cache/torch/NeMo/NeMo_2.4.0rc0/stt_en_fastconformer_hybrid_large_pc/465b32000fc320f5905fda11a1866ef6/stt_en_fastconformer_hybrid_large_pc.nemo.\n",
      "[NeMo I 2025-05-07 17:49:07 hybrid_rnnt_ctc_bpe_models:487] No `decoding_cfg` passed when changing decoding strategy, using internal config\n",
      "[NeMo I 2025-05-07 17:49:07 hybrid_rnnt_ctc_bpe_models:512] Changed decoding strategy of the CTC decoder to \n",
      "    strategy: greedy\n",
      "    preserve_alignments: null\n",
      "    compute_timestamps: null\n",
      "    word_seperator: ' '\n",
      "    segment_seperators:\n",
      "    - .\n",
      "    - '!'\n",
      "    - '?'\n",
      "    segment_gap_threshold: null\n",
      "    ctc_timestamp_type: all\n",
      "    batch_dim_index: 0\n",
      "    greedy:\n",
      "      preserve_alignments: false\n",
      "      compute_timestamps: false\n",
      "      preserve_frame_confidence: false\n",
      "      confidence_method_cfg:\n",
      "        name: entropy\n",
      "        entropy_type: tsallis\n",
      "        alpha: 0.33\n",
      "        entropy_norm: exp\n",
      "        temperature: DEPRECATED\n",
      "    beam:\n",
      "      beam_size: 4\n",
      "      search_type: default\n",
      "      preserve_alignments: false\n",
      "      compute_timestamps: false\n",
      "      return_best_hypothesis: true\n",
      "      beam_alpha: 1.0\n",
      "      beam_beta: 0.0\n",
      "      kenlm_path: null\n",
      "      flashlight_cfg:\n",
      "        lexicon_path: null\n",
      "        boost_path: null\n",
      "        beam_size_token: 16\n",
      "        beam_threshold: 20.0\n",
      "        unk_weight: -.inf\n",
      "        sil_weight: 0.0\n",
      "      pyctcdecode_cfg:\n",
      "        beam_prune_logp: -10.0\n",
      "        token_min_logp: -5.0\n",
      "        prune_history: false\n",
      "        hotwords: null\n",
      "        hotword_weight: 10.0\n",
      "    wfst:\n",
      "      beam_size: 4\n",
      "      search_type: riva\n",
      "      return_best_hypothesis: true\n",
      "      preserve_alignments: false\n",
      "      compute_timestamps: false\n",
      "      decoding_mode: nbest\n",
      "      open_vocabulary_decoding: false\n",
      "      beam_width: 10.0\n",
      "      lm_weight: 1.0\n",
      "      device: cuda\n",
      "      arpa_lm_path: null\n",
      "      wfst_lm_path: null\n",
      "      riva_decoding_cfg: {}\n",
      "      k2_decoding_cfg:\n",
      "        search_beam: 20.0\n",
      "        output_beam: 10.0\n",
      "        min_active_states: 30\n",
      "        max_active_states: 10000\n",
      "    confidence_cfg:\n",
      "      preserve_frame_confidence: false\n",
      "      preserve_token_confidence: false\n",
      "      preserve_word_confidence: false\n",
      "      exclude_blank: true\n",
      "      aggregation: min\n",
      "      tdt_include_duration: false\n",
      "      method_cfg:\n",
      "        name: entropy\n",
      "        entropy_type: tsallis\n",
      "        alpha: 0.33\n",
      "        entropy_norm: exp\n",
      "        temperature: DEPRECATED\n",
      "    temperature: 1.0\n",
      "    \n",
      "[NeMo I 2025-05-07 17:49:07 align:250] Flag use_local_attention is set to True => will try to use local attention for model if it allows it\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Transcribing:   0%|          | 0/1 [00:00<?, ?it/s][NeMo W 2025-05-07 17:50:18 ctc_greedy_decoding:168] CTC decoding strategy 'greedy' is slower than 'greedy_batch', which implements the same exact interface. Consider changing your strategy to 'greedy_batch' for a free performance improvement.\n",
      "Transcribing: 100%|██████████| 1/1 [01:10<00:00, 70.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-05-07 17:50:18 data_prep:828] Calculated that the model downsample factor is 8 and therefore the ASR model output timestep duration is 0.08 -- will use this for all batches\n",
      "Running alignment for: /hdd2/Aman/scripts/ML_assign/Dataset/manifest/case18_2024_11_03_manifest.json\n",
      "[NeMo I 2025-05-07 17:52:46 align:159] Hydra config: pretrained_name: stt_en_fastconformer_hybrid_large_pc\n",
      "    model_path: null\n",
      "    manifest_filepath: /hdd2/Aman/scripts/ML_assign/Dataset/manifest/case18_2024_11_03_manifest.json\n",
      "    output_dir: /hdd2/Aman/scripts/ML_assign/src/nfa_output/case18_2024_11_03_manifest\n",
      "    align_using_pred_text: false\n",
      "    transcribe_device: cpu\n",
      "    viterbi_device: cpu\n",
      "    batch_size: 1\n",
      "    use_local_attention: true\n",
      "    additional_segment_grouping_separator: '|'\n",
      "    audio_filepath_parts_in_utt_id: 1\n",
      "    use_buffered_chunked_streaming: false\n",
      "    chunk_len_in_secs: 1.6\n",
      "    total_buffer_in_secs: 4.0\n",
      "    chunk_batch_size: 32\n",
      "    simulate_cache_aware_streaming: false\n",
      "    save_output_file_formats:\n",
      "    - ctm\n",
      "    - ass\n",
      "    ctm_file_config:\n",
      "      remove_blank_tokens: false\n",
      "      minimum_timestamp_duration: 0.0\n",
      "    ass_file_config:\n",
      "      fontsize: 20\n",
      "      vertical_alignment: center\n",
      "      resegment_text_to_fill_space: false\n",
      "      max_lines_per_segment: 2\n",
      "      text_already_spoken_rgb:\n",
      "      - 49\n",
      "      - 46\n",
      "      - 61\n",
      "      text_being_spoken_rgb:\n",
      "      - 57\n",
      "      - 171\n",
      "      - 9\n",
      "      text_not_yet_spoken_rgb:\n",
      "      - 194\n",
      "      - 193\n",
      "      - 199\n",
      "    \n",
      "[NeMo I 2025-05-07 17:52:46 align:228] Device to be used for transcription step (`transcribe_device`) is cpu\n",
      "[NeMo I 2025-05-07 17:52:46 align:234] Device to be used for viterbi step (`viterbi_device`) is cpu\n",
      "[NeMo I 2025-05-07 17:52:46 cloud:58] Found existing object /home/antpc/.cache/torch/NeMo/NeMo_2.4.0rc0/stt_en_fastconformer_hybrid_large_pc/465b32000fc320f5905fda11a1866ef6/stt_en_fastconformer_hybrid_large_pc.nemo.\n",
      "[NeMo I 2025-05-07 17:52:46 cloud:64] Re-using file from: /home/antpc/.cache/torch/NeMo/NeMo_2.4.0rc0/stt_en_fastconformer_hybrid_large_pc/465b32000fc320f5905fda11a1866ef6/stt_en_fastconformer_hybrid_large_pc.nemo\n",
      "[NeMo I 2025-05-07 17:52:46 common:827] Instantiating model from pre-trained checkpoint\n",
      "[NeMo I 2025-05-07 17:52:47 mixins:181] Tokenizer SentencePieceTokenizer initialized with 1024 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2025-05-07 17:52:48 modelPT:180] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
      "    Train config : \n",
      "    is_concat: false\n",
      "    concat_sampling_technique: random\n",
      "    concat_sampling_temperature: 8\n",
      "    concat_shuffle: true\n",
      "    concat_sampling_scale: 1.0\n",
      "    manifest_filepath:\n",
      "    - - /data/mml/en/tarred_train/pcstrip_sharded_manifests/manifest__OP_0..511_CL_.json\n",
      "    tarred_audio_filepaths:\n",
      "    - - /data/mml/en/tarred_train/audio__OP_0..511_CL_.tar\n",
      "    concat_sampling_probabilities:\n",
      "    - 1.0\n",
      "    sample_rate: 16000\n",
      "    batch_size: 16\n",
      "    shuffle: true\n",
      "    num_workers: 8\n",
      "    pin_memory: true\n",
      "    use_start_end_token: false\n",
      "    trim_silence: false\n",
      "    max_duration: 20\n",
      "    min_duration: 0.1\n",
      "    is_tarred: true\n",
      "    shard_manifests: true\n",
      "    defer_setup: true\n",
      "    shuffle_n: 528387\n",
      "    bucketing_strategy: synced_randomized\n",
      "    bucketing_batch_size: null\n",
      "    \n",
      "[NeMo W 2025-05-07 17:52:48 modelPT:187] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
      "    Validation config : \n",
      "    ds_item:\n",
      "    - name: en_\n",
      "      is_concat: true\n",
      "      concat_sampling_technique: random\n",
      "      concat_sampling_seed: 1234\n",
      "      concat_shuffle: false\n",
      "      concat_sampling_scale: 0.25\n",
      "      concat_sampling_probabilities:\n",
      "      - 0.088929287\n",
      "      - 0.116404906\n",
      "      - 0.101879436\n",
      "      - 0.171855007\n",
      "      - 0.160962192\n",
      "      - 0.092674082\n",
      "      - 0.041152255\n",
      "      - 0.226142835\n",
      "      manifest_filepath:\n",
      "      - - /data/mml/en/val_test/fisher/audio_manifest_dev_clean_pcstrip_en.json\n",
      "        - /data/mml/en/val_test/europarl/audio_manifest_dev_clean_pcstrip_en.json\n",
      "        - /data/mml/en/val_test/librispeech/audio_manifest_dev_clean_pcstrip_en.json\n",
      "        - /data/mml/en/val_test/mcv11/audio_manifest_dev_clean_pcstrip_en.json\n",
      "        - /data/mml/en/val_test/nsc1/audio_manifest_dev_clean_pcstrip_en.json\n",
      "        - /data/mml/en/val_test/SPGI/audio_manifest_dev_clean_pcstrip_en.json\n",
      "        - /data/mml/en/val_test/voxpopuli/audio_manifest_dev_clean_pcstrip_en.json\n",
      "        - /data/mml/en/val_test/mls_dev/mls_dev_pcstrip_en.json\n",
      "      sample_rate: 16000\n",
      "      batch_size: 64\n",
      "      shuffle: false\n",
      "      num_workers: 1\n",
      "      pin_memory: true\n",
      "      max_duration: 20.0\n",
      "      min_duration: 0.1\n",
      "      use_start_end_token: false\n",
      "    - name: fisher_\n",
      "      is_concat: false\n",
      "      manifest_filepath: /data/mml/en/val_test/fisher/audio_manifest_dev_clean_pcstrip_en.json\n",
      "      sample_rate: 16000\n",
      "      batch_size: 64\n",
      "      shuffle: false\n",
      "      num_workers: 1\n",
      "      pin_memory: true\n",
      "      max_duration: 20.0\n",
      "      min_duration: 0.1\n",
      "      use_start_end_token: false\n",
      "    - name: europarl_\n",
      "      is_concat: false\n",
      "      manifest_filepath: /data/mml/en/val_test/europarl/audio_manifest_dev_clean_pcstrip_en.json\n",
      "      sample_rate: 16000\n",
      "      batch_size: 64\n",
      "      shuffle: false\n",
      "      num_workers: 1\n",
      "      pin_memory: true\n",
      "      max_duration: 20.0\n",
      "      min_duration: 0.1\n",
      "      use_start_end_token: false\n",
      "    - name: librispeech_\n",
      "      is_concat: false\n",
      "      manifest_filepath: /data/mml/en/val_test/librispeech/audio_manifest_dev_clean_pcstrip_en.json\n",
      "      sample_rate: 16000\n",
      "      batch_size: 64\n",
      "      shuffle: false\n",
      "      num_workers: 1\n",
      "      pin_memory: true\n",
      "      max_duration: 20.0\n",
      "      min_duration: 0.1\n",
      "      use_start_end_token: false\n",
      "    - name: mcv11_\n",
      "      is_concat: false\n",
      "      manifest_filepath: /data/mml/en/val_test/mcv11/audio_manifest_dev_clean_pcstrip_en.json\n",
      "      sample_rate: 16000\n",
      "      batch_size: 64\n",
      "      shuffle: false\n",
      "      num_workers: 1\n",
      "      pin_memory: true\n",
      "      max_duration: 20.0\n",
      "      min_duration: 0.1\n",
      "      use_start_end_token: false\n",
      "    - name: nsc1_\n",
      "      is_concat: false\n",
      "      manifest_filepath: /data/mml/en/val_test/nsc1/audio_manifest_dev_clean_pcstrip_en.json\n",
      "      sample_rate: 16000\n",
      "      batch_size: 64\n",
      "      shuffle: false\n",
      "      num_workers: 1\n",
      "      pin_memory: true\n",
      "      max_duration: 20.0\n",
      "      min_duration: 0.1\n",
      "      use_start_end_token: false\n",
      "    - name: spgi_\n",
      "      is_concat: false\n",
      "      manifest_filepath: /data/mml/en/val_test/SPGI/audio_manifest_dev_clean_pcstrip_en.json\n",
      "      sample_rate: 16000\n",
      "      batch_size: 64\n",
      "      shuffle: false\n",
      "      num_workers: 1\n",
      "      pin_memory: true\n",
      "      max_duration: 20.0\n",
      "      min_duration: 0.1\n",
      "      use_start_end_token: false\n",
      "    - name: voxpopuli_\n",
      "      is_concat: false\n",
      "      manifest_filepath: /data/mml/en/val_test/voxpopuli/audio_manifest_dev_clean_pcstrip_en.json\n",
      "      sample_rate: 16000\n",
      "      batch_size: 64\n",
      "      shuffle: false\n",
      "      num_workers: 1\n",
      "      pin_memory: true\n",
      "      max_duration: 20.0\n",
      "      min_duration: 0.1\n",
      "      use_start_end_token: false\n",
      "    - name: mls_\n",
      "      is_concat: false\n",
      "      manifest_filepath: /data/mml/en/val_test/mls_dev/mls_dev_pcstrip_en.json\n",
      "      sample_rate: 16000\n",
      "      batch_size: 64\n",
      "      shuffle: false\n",
      "      num_workers: 1\n",
      "      pin_memory: true\n",
      "      max_duration: 20.0\n",
      "      min_duration: 0.1\n",
      "      use_start_end_token: false\n",
      "    max_duration: 20\n",
      "    \n",
      "[NeMo W 2025-05-07 17:52:48 modelPT:194] Please call the ModelPT.setup_test_data() or ModelPT.setup_multiple_test_data() method and provide a valid configuration file to setup the test data loader(s).\n",
      "    Test config : \n",
      "    manifest_filepath: null\n",
      "    sample_rate: 16000\n",
      "    batch_size: 16\n",
      "    shuffle: false\n",
      "    num_workers: 8\n",
      "    pin_memory: true\n",
      "    use_start_end_token: false\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-05-07 17:52:48 features:305] PADDING: 0\n",
      "[NeMo I 2025-05-07 17:52:50 rnnt_models:226] Using RNNT Loss : warprnnt_numba\n",
      "    Loss warprnnt_numba_kwargs: {'fastemit_lambda': 0.0, 'clamp': -1.0}\n",
      "[NeMo I 2025-05-07 17:52:50 rnnt_models:226] Using RNNT Loss : warprnnt_numba\n",
      "    Loss warprnnt_numba_kwargs: {'fastemit_lambda': 0.0, 'clamp': -1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2025-05-07 17:52:50 rnnt_loop_labels_computer:290] No conditional node support for Cuda.\n",
      "    Cuda graphs with while loops are disabled, decoding speed will be slower\n",
      "    Reason: Driver supports cuda toolkit version 12.2, but the driver needs to support at least 12,3. Please update your cuda driver.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-05-07 17:52:50 rnnt_models:226] Using RNNT Loss : warprnnt_numba\n",
      "    Loss warprnnt_numba_kwargs: {'fastemit_lambda': 0.0, 'clamp': -1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2025-05-07 17:52:50 rnnt_loop_labels_computer:290] No conditional node support for Cuda.\n",
      "    Cuda graphs with while loops are disabled, decoding speed will be slower\n",
      "    Reason: Driver supports cuda toolkit version 12.2, but the driver needs to support at least 12,3. Please update your cuda driver.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-05-07 17:52:51 save_restore_connector:275] Model EncDecHybridRNNTCTCBPEModel was successfully restored from /home/antpc/.cache/torch/NeMo/NeMo_2.4.0rc0/stt_en_fastconformer_hybrid_large_pc/465b32000fc320f5905fda11a1866ef6/stt_en_fastconformer_hybrid_large_pc.nemo.\n",
      "[NeMo I 2025-05-07 17:52:51 hybrid_rnnt_ctc_bpe_models:487] No `decoding_cfg` passed when changing decoding strategy, using internal config\n",
      "[NeMo I 2025-05-07 17:52:51 hybrid_rnnt_ctc_bpe_models:512] Changed decoding strategy of the CTC decoder to \n",
      "    strategy: greedy\n",
      "    preserve_alignments: null\n",
      "    compute_timestamps: null\n",
      "    word_seperator: ' '\n",
      "    segment_seperators:\n",
      "    - .\n",
      "    - '!'\n",
      "    - '?'\n",
      "    segment_gap_threshold: null\n",
      "    ctc_timestamp_type: all\n",
      "    batch_dim_index: 0\n",
      "    greedy:\n",
      "      preserve_alignments: false\n",
      "      compute_timestamps: false\n",
      "      preserve_frame_confidence: false\n",
      "      confidence_method_cfg:\n",
      "        name: entropy\n",
      "        entropy_type: tsallis\n",
      "        alpha: 0.33\n",
      "        entropy_norm: exp\n",
      "        temperature: DEPRECATED\n",
      "    beam:\n",
      "      beam_size: 4\n",
      "      search_type: default\n",
      "      preserve_alignments: false\n",
      "      compute_timestamps: false\n",
      "      return_best_hypothesis: true\n",
      "      beam_alpha: 1.0\n",
      "      beam_beta: 0.0\n",
      "      kenlm_path: null\n",
      "      flashlight_cfg:\n",
      "        lexicon_path: null\n",
      "        boost_path: null\n",
      "        beam_size_token: 16\n",
      "        beam_threshold: 20.0\n",
      "        unk_weight: -.inf\n",
      "        sil_weight: 0.0\n",
      "      pyctcdecode_cfg:\n",
      "        beam_prune_logp: -10.0\n",
      "        token_min_logp: -5.0\n",
      "        prune_history: false\n",
      "        hotwords: null\n",
      "        hotword_weight: 10.0\n",
      "    wfst:\n",
      "      beam_size: 4\n",
      "      search_type: riva\n",
      "      return_best_hypothesis: true\n",
      "      preserve_alignments: false\n",
      "      compute_timestamps: false\n",
      "      decoding_mode: nbest\n",
      "      open_vocabulary_decoding: false\n",
      "      beam_width: 10.0\n",
      "      lm_weight: 1.0\n",
      "      device: cuda\n",
      "      arpa_lm_path: null\n",
      "      wfst_lm_path: null\n",
      "      riva_decoding_cfg: {}\n",
      "      k2_decoding_cfg:\n",
      "        search_beam: 20.0\n",
      "        output_beam: 10.0\n",
      "        min_active_states: 30\n",
      "        max_active_states: 10000\n",
      "    confidence_cfg:\n",
      "      preserve_frame_confidence: false\n",
      "      preserve_token_confidence: false\n",
      "      preserve_word_confidence: false\n",
      "      exclude_blank: true\n",
      "      aggregation: min\n",
      "      tdt_include_duration: false\n",
      "      method_cfg:\n",
      "        name: entropy\n",
      "        entropy_type: tsallis\n",
      "        alpha: 0.33\n",
      "        entropy_norm: exp\n",
      "        temperature: DEPRECATED\n",
      "    temperature: 1.0\n",
      "    \n",
      "[NeMo I 2025-05-07 17:52:51 align:250] Flag use_local_attention is set to True => will try to use local attention for model if it allows it\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Transcribing:   0%|          | 0/1 [00:00<?, ?it/s][NeMo W 2025-05-07 17:53:10 ctc_greedy_decoding:168] CTC decoding strategy 'greedy' is slower than 'greedy_batch', which implements the same exact interface. Consider changing your strategy to 'greedy_batch' for a free performance improvement.\n",
      "Transcribing: 100%|██████████| 1/1 [00:19<00:00, 19.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-05-07 17:53:10 data_prep:828] Calculated that the model downsample factor is 8 and therefore the ASR model output timestep duration is 0.08 -- will use this for all batches\n",
      "Running alignment for: /hdd2/Aman/scripts/ML_assign/Dataset/manifest/case1_2023_12_12_manifest.json\n",
      "[NeMo I 2025-05-07 17:53:36 align:159] Hydra config: pretrained_name: stt_en_fastconformer_hybrid_large_pc\n",
      "    model_path: null\n",
      "    manifest_filepath: /hdd2/Aman/scripts/ML_assign/Dataset/manifest/case1_2023_12_12_manifest.json\n",
      "    output_dir: /hdd2/Aman/scripts/ML_assign/src/nfa_output/case1_2023_12_12_manifest\n",
      "    align_using_pred_text: false\n",
      "    transcribe_device: cpu\n",
      "    viterbi_device: cpu\n",
      "    batch_size: 1\n",
      "    use_local_attention: true\n",
      "    additional_segment_grouping_separator: '|'\n",
      "    audio_filepath_parts_in_utt_id: 1\n",
      "    use_buffered_chunked_streaming: false\n",
      "    chunk_len_in_secs: 1.6\n",
      "    total_buffer_in_secs: 4.0\n",
      "    chunk_batch_size: 32\n",
      "    simulate_cache_aware_streaming: false\n",
      "    save_output_file_formats:\n",
      "    - ctm\n",
      "    - ass\n",
      "    ctm_file_config:\n",
      "      remove_blank_tokens: false\n",
      "      minimum_timestamp_duration: 0.0\n",
      "    ass_file_config:\n",
      "      fontsize: 20\n",
      "      vertical_alignment: center\n",
      "      resegment_text_to_fill_space: false\n",
      "      max_lines_per_segment: 2\n",
      "      text_already_spoken_rgb:\n",
      "      - 49\n",
      "      - 46\n",
      "      - 61\n",
      "      text_being_spoken_rgb:\n",
      "      - 57\n",
      "      - 171\n",
      "      - 9\n",
      "      text_not_yet_spoken_rgb:\n",
      "      - 194\n",
      "      - 193\n",
      "      - 199\n",
      "    \n",
      "[NeMo I 2025-05-07 17:53:36 align:228] Device to be used for transcription step (`transcribe_device`) is cpu\n",
      "[NeMo I 2025-05-07 17:53:36 align:234] Device to be used for viterbi step (`viterbi_device`) is cpu\n",
      "[NeMo I 2025-05-07 17:53:36 cloud:58] Found existing object /home/antpc/.cache/torch/NeMo/NeMo_2.4.0rc0/stt_en_fastconformer_hybrid_large_pc/465b32000fc320f5905fda11a1866ef6/stt_en_fastconformer_hybrid_large_pc.nemo.\n",
      "[NeMo I 2025-05-07 17:53:36 cloud:64] Re-using file from: /home/antpc/.cache/torch/NeMo/NeMo_2.4.0rc0/stt_en_fastconformer_hybrid_large_pc/465b32000fc320f5905fda11a1866ef6/stt_en_fastconformer_hybrid_large_pc.nemo\n",
      "[NeMo I 2025-05-07 17:53:36 common:827] Instantiating model from pre-trained checkpoint\n",
      "[NeMo I 2025-05-07 17:53:38 mixins:181] Tokenizer SentencePieceTokenizer initialized with 1024 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2025-05-07 17:53:39 modelPT:180] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
      "    Train config : \n",
      "    is_concat: false\n",
      "    concat_sampling_technique: random\n",
      "    concat_sampling_temperature: 8\n",
      "    concat_shuffle: true\n",
      "    concat_sampling_scale: 1.0\n",
      "    manifest_filepath:\n",
      "    - - /data/mml/en/tarred_train/pcstrip_sharded_manifests/manifest__OP_0..511_CL_.json\n",
      "    tarred_audio_filepaths:\n",
      "    - - /data/mml/en/tarred_train/audio__OP_0..511_CL_.tar\n",
      "    concat_sampling_probabilities:\n",
      "    - 1.0\n",
      "    sample_rate: 16000\n",
      "    batch_size: 16\n",
      "    shuffle: true\n",
      "    num_workers: 8\n",
      "    pin_memory: true\n",
      "    use_start_end_token: false\n",
      "    trim_silence: false\n",
      "    max_duration: 20\n",
      "    min_duration: 0.1\n",
      "    is_tarred: true\n",
      "    shard_manifests: true\n",
      "    defer_setup: true\n",
      "    shuffle_n: 528387\n",
      "    bucketing_strategy: synced_randomized\n",
      "    bucketing_batch_size: null\n",
      "    \n",
      "[NeMo W 2025-05-07 17:53:39 modelPT:187] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
      "    Validation config : \n",
      "    ds_item:\n",
      "    - name: en_\n",
      "      is_concat: true\n",
      "      concat_sampling_technique: random\n",
      "      concat_sampling_seed: 1234\n",
      "      concat_shuffle: false\n",
      "      concat_sampling_scale: 0.25\n",
      "      concat_sampling_probabilities:\n",
      "      - 0.088929287\n",
      "      - 0.116404906\n",
      "      - 0.101879436\n",
      "      - 0.171855007\n",
      "      - 0.160962192\n",
      "      - 0.092674082\n",
      "      - 0.041152255\n",
      "      - 0.226142835\n",
      "      manifest_filepath:\n",
      "      - - /data/mml/en/val_test/fisher/audio_manifest_dev_clean_pcstrip_en.json\n",
      "        - /data/mml/en/val_test/europarl/audio_manifest_dev_clean_pcstrip_en.json\n",
      "        - /data/mml/en/val_test/librispeech/audio_manifest_dev_clean_pcstrip_en.json\n",
      "        - /data/mml/en/val_test/mcv11/audio_manifest_dev_clean_pcstrip_en.json\n",
      "        - /data/mml/en/val_test/nsc1/audio_manifest_dev_clean_pcstrip_en.json\n",
      "        - /data/mml/en/val_test/SPGI/audio_manifest_dev_clean_pcstrip_en.json\n",
      "        - /data/mml/en/val_test/voxpopuli/audio_manifest_dev_clean_pcstrip_en.json\n",
      "        - /data/mml/en/val_test/mls_dev/mls_dev_pcstrip_en.json\n",
      "      sample_rate: 16000\n",
      "      batch_size: 64\n",
      "      shuffle: false\n",
      "      num_workers: 1\n",
      "      pin_memory: true\n",
      "      max_duration: 20.0\n",
      "      min_duration: 0.1\n",
      "      use_start_end_token: false\n",
      "    - name: fisher_\n",
      "      is_concat: false\n",
      "      manifest_filepath: /data/mml/en/val_test/fisher/audio_manifest_dev_clean_pcstrip_en.json\n",
      "      sample_rate: 16000\n",
      "      batch_size: 64\n",
      "      shuffle: false\n",
      "      num_workers: 1\n",
      "      pin_memory: true\n",
      "      max_duration: 20.0\n",
      "      min_duration: 0.1\n",
      "      use_start_end_token: false\n",
      "    - name: europarl_\n",
      "      is_concat: false\n",
      "      manifest_filepath: /data/mml/en/val_test/europarl/audio_manifest_dev_clean_pcstrip_en.json\n",
      "      sample_rate: 16000\n",
      "      batch_size: 64\n",
      "      shuffle: false\n",
      "      num_workers: 1\n",
      "      pin_memory: true\n",
      "      max_duration: 20.0\n",
      "      min_duration: 0.1\n",
      "      use_start_end_token: false\n",
      "    - name: librispeech_\n",
      "      is_concat: false\n",
      "      manifest_filepath: /data/mml/en/val_test/librispeech/audio_manifest_dev_clean_pcstrip_en.json\n",
      "      sample_rate: 16000\n",
      "      batch_size: 64\n",
      "      shuffle: false\n",
      "      num_workers: 1\n",
      "      pin_memory: true\n",
      "      max_duration: 20.0\n",
      "      min_duration: 0.1\n",
      "      use_start_end_token: false\n",
      "    - name: mcv11_\n",
      "      is_concat: false\n",
      "      manifest_filepath: /data/mml/en/val_test/mcv11/audio_manifest_dev_clean_pcstrip_en.json\n",
      "      sample_rate: 16000\n",
      "      batch_size: 64\n",
      "      shuffle: false\n",
      "      num_workers: 1\n",
      "      pin_memory: true\n",
      "      max_duration: 20.0\n",
      "      min_duration: 0.1\n",
      "      use_start_end_token: false\n",
      "    - name: nsc1_\n",
      "      is_concat: false\n",
      "      manifest_filepath: /data/mml/en/val_test/nsc1/audio_manifest_dev_clean_pcstrip_en.json\n",
      "      sample_rate: 16000\n",
      "      batch_size: 64\n",
      "      shuffle: false\n",
      "      num_workers: 1\n",
      "      pin_memory: true\n",
      "      max_duration: 20.0\n",
      "      min_duration: 0.1\n",
      "      use_start_end_token: false\n",
      "    - name: spgi_\n",
      "      is_concat: false\n",
      "      manifest_filepath: /data/mml/en/val_test/SPGI/audio_manifest_dev_clean_pcstrip_en.json\n",
      "      sample_rate: 16000\n",
      "      batch_size: 64\n",
      "      shuffle: false\n",
      "      num_workers: 1\n",
      "      pin_memory: true\n",
      "      max_duration: 20.0\n",
      "      min_duration: 0.1\n",
      "      use_start_end_token: false\n",
      "    - name: voxpopuli_\n",
      "      is_concat: false\n",
      "      manifest_filepath: /data/mml/en/val_test/voxpopuli/audio_manifest_dev_clean_pcstrip_en.json\n",
      "      sample_rate: 16000\n",
      "      batch_size: 64\n",
      "      shuffle: false\n",
      "      num_workers: 1\n",
      "      pin_memory: true\n",
      "      max_duration: 20.0\n",
      "      min_duration: 0.1\n",
      "      use_start_end_token: false\n",
      "    - name: mls_\n",
      "      is_concat: false\n",
      "      manifest_filepath: /data/mml/en/val_test/mls_dev/mls_dev_pcstrip_en.json\n",
      "      sample_rate: 16000\n",
      "      batch_size: 64\n",
      "      shuffle: false\n",
      "      num_workers: 1\n",
      "      pin_memory: true\n",
      "      max_duration: 20.0\n",
      "      min_duration: 0.1\n",
      "      use_start_end_token: false\n",
      "    max_duration: 20\n",
      "    \n",
      "[NeMo W 2025-05-07 17:53:39 modelPT:194] Please call the ModelPT.setup_test_data() or ModelPT.setup_multiple_test_data() method and provide a valid configuration file to setup the test data loader(s).\n",
      "    Test config : \n",
      "    manifest_filepath: null\n",
      "    sample_rate: 16000\n",
      "    batch_size: 16\n",
      "    shuffle: false\n",
      "    num_workers: 8\n",
      "    pin_memory: true\n",
      "    use_start_end_token: false\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-05-07 17:53:39 features:305] PADDING: 0\n",
      "[NeMo I 2025-05-07 17:53:40 rnnt_models:226] Using RNNT Loss : warprnnt_numba\n",
      "    Loss warprnnt_numba_kwargs: {'fastemit_lambda': 0.0, 'clamp': -1.0}\n",
      "[NeMo I 2025-05-07 17:53:40 rnnt_models:226] Using RNNT Loss : warprnnt_numba\n",
      "    Loss warprnnt_numba_kwargs: {'fastemit_lambda': 0.0, 'clamp': -1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2025-05-07 17:53:40 rnnt_loop_labels_computer:290] No conditional node support for Cuda.\n",
      "    Cuda graphs with while loops are disabled, decoding speed will be slower\n",
      "    Reason: Driver supports cuda toolkit version 12.2, but the driver needs to support at least 12,3. Please update your cuda driver.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-05-07 17:53:41 rnnt_models:226] Using RNNT Loss : warprnnt_numba\n",
      "    Loss warprnnt_numba_kwargs: {'fastemit_lambda': 0.0, 'clamp': -1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2025-05-07 17:53:41 rnnt_loop_labels_computer:290] No conditional node support for Cuda.\n",
      "    Cuda graphs with while loops are disabled, decoding speed will be slower\n",
      "    Reason: Driver supports cuda toolkit version 12.2, but the driver needs to support at least 12,3. Please update your cuda driver.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-05-07 17:53:41 save_restore_connector:275] Model EncDecHybridRNNTCTCBPEModel was successfully restored from /home/antpc/.cache/torch/NeMo/NeMo_2.4.0rc0/stt_en_fastconformer_hybrid_large_pc/465b32000fc320f5905fda11a1866ef6/stt_en_fastconformer_hybrid_large_pc.nemo.\n",
      "[NeMo I 2025-05-07 17:53:41 hybrid_rnnt_ctc_bpe_models:487] No `decoding_cfg` passed when changing decoding strategy, using internal config\n",
      "[NeMo I 2025-05-07 17:53:41 hybrid_rnnt_ctc_bpe_models:512] Changed decoding strategy of the CTC decoder to \n",
      "    strategy: greedy\n",
      "    preserve_alignments: null\n",
      "    compute_timestamps: null\n",
      "    word_seperator: ' '\n",
      "    segment_seperators:\n",
      "    - .\n",
      "    - '!'\n",
      "    - '?'\n",
      "    segment_gap_threshold: null\n",
      "    ctc_timestamp_type: all\n",
      "    batch_dim_index: 0\n",
      "    greedy:\n",
      "      preserve_alignments: false\n",
      "      compute_timestamps: false\n",
      "      preserve_frame_confidence: false\n",
      "      confidence_method_cfg:\n",
      "        name: entropy\n",
      "        entropy_type: tsallis\n",
      "        alpha: 0.33\n",
      "        entropy_norm: exp\n",
      "        temperature: DEPRECATED\n",
      "    beam:\n",
      "      beam_size: 4\n",
      "      search_type: default\n",
      "      preserve_alignments: false\n",
      "      compute_timestamps: false\n",
      "      return_best_hypothesis: true\n",
      "      beam_alpha: 1.0\n",
      "      beam_beta: 0.0\n",
      "      kenlm_path: null\n",
      "      flashlight_cfg:\n",
      "        lexicon_path: null\n",
      "        boost_path: null\n",
      "        beam_size_token: 16\n",
      "        beam_threshold: 20.0\n",
      "        unk_weight: -.inf\n",
      "        sil_weight: 0.0\n",
      "      pyctcdecode_cfg:\n",
      "        beam_prune_logp: -10.0\n",
      "        token_min_logp: -5.0\n",
      "        prune_history: false\n",
      "        hotwords: null\n",
      "        hotword_weight: 10.0\n",
      "    wfst:\n",
      "      beam_size: 4\n",
      "      search_type: riva\n",
      "      return_best_hypothesis: true\n",
      "      preserve_alignments: false\n",
      "      compute_timestamps: false\n",
      "      decoding_mode: nbest\n",
      "      open_vocabulary_decoding: false\n",
      "      beam_width: 10.0\n",
      "      lm_weight: 1.0\n",
      "      device: cuda\n",
      "      arpa_lm_path: null\n",
      "      wfst_lm_path: null\n",
      "      riva_decoding_cfg: {}\n",
      "      k2_decoding_cfg:\n",
      "        search_beam: 20.0\n",
      "        output_beam: 10.0\n",
      "        min_active_states: 30\n",
      "        max_active_states: 10000\n",
      "    confidence_cfg:\n",
      "      preserve_frame_confidence: false\n",
      "      preserve_token_confidence: false\n",
      "      preserve_word_confidence: false\n",
      "      exclude_blank: true\n",
      "      aggregation: min\n",
      "      tdt_include_duration: false\n",
      "      method_cfg:\n",
      "        name: entropy\n",
      "        entropy_type: tsallis\n",
      "        alpha: 0.33\n",
      "        entropy_norm: exp\n",
      "        temperature: DEPRECATED\n",
      "    temperature: 1.0\n",
      "    \n",
      "[NeMo I 2025-05-07 17:53:41 align:250] Flag use_local_attention is set to True => will try to use local attention for model if it allows it\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Transcribing:   0%|          | 0/1 [00:00<?, ?it/s][NeMo W 2025-05-07 17:55:30 ctc_greedy_decoding:168] CTC decoding strategy 'greedy' is slower than 'greedy_batch', which implements the same exact interface. Consider changing your strategy to 'greedy_batch' for a free performance improvement.\n",
      "Transcribing: 100%|██████████| 1/1 [01:48<00:00, 108.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-05-07 17:55:31 data_prep:828] Calculated that the model downsample factor is 8 and therefore the ASR model output timestep duration is 0.08 -- will use this for all batches\n",
      "Running alignment for: /hdd2/Aman/scripts/ML_assign/Dataset/manifest/case10_2023_02_21_manifest.json\n",
      "[NeMo I 2025-05-07 17:58:44 align:159] Hydra config: pretrained_name: stt_en_fastconformer_hybrid_large_pc\n",
      "    model_path: null\n",
      "    manifest_filepath: /hdd2/Aman/scripts/ML_assign/Dataset/manifest/case10_2023_02_21_manifest.json\n",
      "    output_dir: /hdd2/Aman/scripts/ML_assign/src/nfa_output/case10_2023_02_21_manifest\n",
      "    align_using_pred_text: false\n",
      "    transcribe_device: cpu\n",
      "    viterbi_device: cpu\n",
      "    batch_size: 1\n",
      "    use_local_attention: true\n",
      "    additional_segment_grouping_separator: '|'\n",
      "    audio_filepath_parts_in_utt_id: 1\n",
      "    use_buffered_chunked_streaming: false\n",
      "    chunk_len_in_secs: 1.6\n",
      "    total_buffer_in_secs: 4.0\n",
      "    chunk_batch_size: 32\n",
      "    simulate_cache_aware_streaming: false\n",
      "    save_output_file_formats:\n",
      "    - ctm\n",
      "    - ass\n",
      "    ctm_file_config:\n",
      "      remove_blank_tokens: false\n",
      "      minimum_timestamp_duration: 0.0\n",
      "    ass_file_config:\n",
      "      fontsize: 20\n",
      "      vertical_alignment: center\n",
      "      resegment_text_to_fill_space: false\n",
      "      max_lines_per_segment: 2\n",
      "      text_already_spoken_rgb:\n",
      "      - 49\n",
      "      - 46\n",
      "      - 61\n",
      "      text_being_spoken_rgb:\n",
      "      - 57\n",
      "      - 171\n",
      "      - 9\n",
      "      text_not_yet_spoken_rgb:\n",
      "      - 194\n",
      "      - 193\n",
      "      - 199\n",
      "    \n",
      "[NeMo I 2025-05-07 17:58:44 align:228] Device to be used for transcription step (`transcribe_device`) is cpu\n",
      "[NeMo I 2025-05-07 17:58:44 align:234] Device to be used for viterbi step (`viterbi_device`) is cpu\n",
      "[NeMo I 2025-05-07 17:58:44 cloud:58] Found existing object /home/antpc/.cache/torch/NeMo/NeMo_2.4.0rc0/stt_en_fastconformer_hybrid_large_pc/465b32000fc320f5905fda11a1866ef6/stt_en_fastconformer_hybrid_large_pc.nemo.\n",
      "[NeMo I 2025-05-07 17:58:44 cloud:64] Re-using file from: /home/antpc/.cache/torch/NeMo/NeMo_2.4.0rc0/stt_en_fastconformer_hybrid_large_pc/465b32000fc320f5905fda11a1866ef6/stt_en_fastconformer_hybrid_large_pc.nemo\n",
      "[NeMo I 2025-05-07 17:58:44 common:827] Instantiating model from pre-trained checkpoint\n",
      "[NeMo I 2025-05-07 17:58:46 mixins:181] Tokenizer SentencePieceTokenizer initialized with 1024 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2025-05-07 17:58:47 modelPT:180] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
      "    Train config : \n",
      "    is_concat: false\n",
      "    concat_sampling_technique: random\n",
      "    concat_sampling_temperature: 8\n",
      "    concat_shuffle: true\n",
      "    concat_sampling_scale: 1.0\n",
      "    manifest_filepath:\n",
      "    - - /data/mml/en/tarred_train/pcstrip_sharded_manifests/manifest__OP_0..511_CL_.json\n",
      "    tarred_audio_filepaths:\n",
      "    - - /data/mml/en/tarred_train/audio__OP_0..511_CL_.tar\n",
      "    concat_sampling_probabilities:\n",
      "    - 1.0\n",
      "    sample_rate: 16000\n",
      "    batch_size: 16\n",
      "    shuffle: true\n",
      "    num_workers: 8\n",
      "    pin_memory: true\n",
      "    use_start_end_token: false\n",
      "    trim_silence: false\n",
      "    max_duration: 20\n",
      "    min_duration: 0.1\n",
      "    is_tarred: true\n",
      "    shard_manifests: true\n",
      "    defer_setup: true\n",
      "    shuffle_n: 528387\n",
      "    bucketing_strategy: synced_randomized\n",
      "    bucketing_batch_size: null\n",
      "    \n",
      "[NeMo W 2025-05-07 17:58:47 modelPT:187] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
      "    Validation config : \n",
      "    ds_item:\n",
      "    - name: en_\n",
      "      is_concat: true\n",
      "      concat_sampling_technique: random\n",
      "      concat_sampling_seed: 1234\n",
      "      concat_shuffle: false\n",
      "      concat_sampling_scale: 0.25\n",
      "      concat_sampling_probabilities:\n",
      "      - 0.088929287\n",
      "      - 0.116404906\n",
      "      - 0.101879436\n",
      "      - 0.171855007\n",
      "      - 0.160962192\n",
      "      - 0.092674082\n",
      "      - 0.041152255\n",
      "      - 0.226142835\n",
      "      manifest_filepath:\n",
      "      - - /data/mml/en/val_test/fisher/audio_manifest_dev_clean_pcstrip_en.json\n",
      "        - /data/mml/en/val_test/europarl/audio_manifest_dev_clean_pcstrip_en.json\n",
      "        - /data/mml/en/val_test/librispeech/audio_manifest_dev_clean_pcstrip_en.json\n",
      "        - /data/mml/en/val_test/mcv11/audio_manifest_dev_clean_pcstrip_en.json\n",
      "        - /data/mml/en/val_test/nsc1/audio_manifest_dev_clean_pcstrip_en.json\n",
      "        - /data/mml/en/val_test/SPGI/audio_manifest_dev_clean_pcstrip_en.json\n",
      "        - /data/mml/en/val_test/voxpopuli/audio_manifest_dev_clean_pcstrip_en.json\n",
      "        - /data/mml/en/val_test/mls_dev/mls_dev_pcstrip_en.json\n",
      "      sample_rate: 16000\n",
      "      batch_size: 64\n",
      "      shuffle: false\n",
      "      num_workers: 1\n",
      "      pin_memory: true\n",
      "      max_duration: 20.0\n",
      "      min_duration: 0.1\n",
      "      use_start_end_token: false\n",
      "    - name: fisher_\n",
      "      is_concat: false\n",
      "      manifest_filepath: /data/mml/en/val_test/fisher/audio_manifest_dev_clean_pcstrip_en.json\n",
      "      sample_rate: 16000\n",
      "      batch_size: 64\n",
      "      shuffle: false\n",
      "      num_workers: 1\n",
      "      pin_memory: true\n",
      "      max_duration: 20.0\n",
      "      min_duration: 0.1\n",
      "      use_start_end_token: false\n",
      "    - name: europarl_\n",
      "      is_concat: false\n",
      "      manifest_filepath: /data/mml/en/val_test/europarl/audio_manifest_dev_clean_pcstrip_en.json\n",
      "      sample_rate: 16000\n",
      "      batch_size: 64\n",
      "      shuffle: false\n",
      "      num_workers: 1\n",
      "      pin_memory: true\n",
      "      max_duration: 20.0\n",
      "      min_duration: 0.1\n",
      "      use_start_end_token: false\n",
      "    - name: librispeech_\n",
      "      is_concat: false\n",
      "      manifest_filepath: /data/mml/en/val_test/librispeech/audio_manifest_dev_clean_pcstrip_en.json\n",
      "      sample_rate: 16000\n",
      "      batch_size: 64\n",
      "      shuffle: false\n",
      "      num_workers: 1\n",
      "      pin_memory: true\n",
      "      max_duration: 20.0\n",
      "      min_duration: 0.1\n",
      "      use_start_end_token: false\n",
      "    - name: mcv11_\n",
      "      is_concat: false\n",
      "      manifest_filepath: /data/mml/en/val_test/mcv11/audio_manifest_dev_clean_pcstrip_en.json\n",
      "      sample_rate: 16000\n",
      "      batch_size: 64\n",
      "      shuffle: false\n",
      "      num_workers: 1\n",
      "      pin_memory: true\n",
      "      max_duration: 20.0\n",
      "      min_duration: 0.1\n",
      "      use_start_end_token: false\n",
      "    - name: nsc1_\n",
      "      is_concat: false\n",
      "      manifest_filepath: /data/mml/en/val_test/nsc1/audio_manifest_dev_clean_pcstrip_en.json\n",
      "      sample_rate: 16000\n",
      "      batch_size: 64\n",
      "      shuffle: false\n",
      "      num_workers: 1\n",
      "      pin_memory: true\n",
      "      max_duration: 20.0\n",
      "      min_duration: 0.1\n",
      "      use_start_end_token: false\n",
      "    - name: spgi_\n",
      "      is_concat: false\n",
      "      manifest_filepath: /data/mml/en/val_test/SPGI/audio_manifest_dev_clean_pcstrip_en.json\n",
      "      sample_rate: 16000\n",
      "      batch_size: 64\n",
      "      shuffle: false\n",
      "      num_workers: 1\n",
      "      pin_memory: true\n",
      "      max_duration: 20.0\n",
      "      min_duration: 0.1\n",
      "      use_start_end_token: false\n",
      "    - name: voxpopuli_\n",
      "      is_concat: false\n",
      "      manifest_filepath: /data/mml/en/val_test/voxpopuli/audio_manifest_dev_clean_pcstrip_en.json\n",
      "      sample_rate: 16000\n",
      "      batch_size: 64\n",
      "      shuffle: false\n",
      "      num_workers: 1\n",
      "      pin_memory: true\n",
      "      max_duration: 20.0\n",
      "      min_duration: 0.1\n",
      "      use_start_end_token: false\n",
      "    - name: mls_\n",
      "      is_concat: false\n",
      "      manifest_filepath: /data/mml/en/val_test/mls_dev/mls_dev_pcstrip_en.json\n",
      "      sample_rate: 16000\n",
      "      batch_size: 64\n",
      "      shuffle: false\n",
      "      num_workers: 1\n",
      "      pin_memory: true\n",
      "      max_duration: 20.0\n",
      "      min_duration: 0.1\n",
      "      use_start_end_token: false\n",
      "    max_duration: 20\n",
      "    \n",
      "[NeMo W 2025-05-07 17:58:47 modelPT:194] Please call the ModelPT.setup_test_data() or ModelPT.setup_multiple_test_data() method and provide a valid configuration file to setup the test data loader(s).\n",
      "    Test config : \n",
      "    manifest_filepath: null\n",
      "    sample_rate: 16000\n",
      "    batch_size: 16\n",
      "    shuffle: false\n",
      "    num_workers: 8\n",
      "    pin_memory: true\n",
      "    use_start_end_token: false\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-05-07 17:58:47 features:305] PADDING: 0\n",
      "[NeMo I 2025-05-07 17:58:48 rnnt_models:226] Using RNNT Loss : warprnnt_numba\n",
      "    Loss warprnnt_numba_kwargs: {'fastemit_lambda': 0.0, 'clamp': -1.0}\n",
      "[NeMo I 2025-05-07 17:58:48 rnnt_models:226] Using RNNT Loss : warprnnt_numba\n",
      "    Loss warprnnt_numba_kwargs: {'fastemit_lambda': 0.0, 'clamp': -1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2025-05-07 17:58:49 rnnt_loop_labels_computer:290] No conditional node support for Cuda.\n",
      "    Cuda graphs with while loops are disabled, decoding speed will be slower\n",
      "    Reason: Driver supports cuda toolkit version 12.2, but the driver needs to support at least 12,3. Please update your cuda driver.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-05-07 17:58:49 rnnt_models:226] Using RNNT Loss : warprnnt_numba\n",
      "    Loss warprnnt_numba_kwargs: {'fastemit_lambda': 0.0, 'clamp': -1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2025-05-07 17:58:49 rnnt_loop_labels_computer:290] No conditional node support for Cuda.\n",
      "    Cuda graphs with while loops are disabled, decoding speed will be slower\n",
      "    Reason: Driver supports cuda toolkit version 12.2, but the driver needs to support at least 12,3. Please update your cuda driver.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-05-07 17:58:49 save_restore_connector:275] Model EncDecHybridRNNTCTCBPEModel was successfully restored from /home/antpc/.cache/torch/NeMo/NeMo_2.4.0rc0/stt_en_fastconformer_hybrid_large_pc/465b32000fc320f5905fda11a1866ef6/stt_en_fastconformer_hybrid_large_pc.nemo.\n",
      "[NeMo I 2025-05-07 17:58:49 hybrid_rnnt_ctc_bpe_models:487] No `decoding_cfg` passed when changing decoding strategy, using internal config\n",
      "[NeMo I 2025-05-07 17:58:49 hybrid_rnnt_ctc_bpe_models:512] Changed decoding strategy of the CTC decoder to \n",
      "    strategy: greedy\n",
      "    preserve_alignments: null\n",
      "    compute_timestamps: null\n",
      "    word_seperator: ' '\n",
      "    segment_seperators:\n",
      "    - .\n",
      "    - '!'\n",
      "    - '?'\n",
      "    segment_gap_threshold: null\n",
      "    ctc_timestamp_type: all\n",
      "    batch_dim_index: 0\n",
      "    greedy:\n",
      "      preserve_alignments: false\n",
      "      compute_timestamps: false\n",
      "      preserve_frame_confidence: false\n",
      "      confidence_method_cfg:\n",
      "        name: entropy\n",
      "        entropy_type: tsallis\n",
      "        alpha: 0.33\n",
      "        entropy_norm: exp\n",
      "        temperature: DEPRECATED\n",
      "    beam:\n",
      "      beam_size: 4\n",
      "      search_type: default\n",
      "      preserve_alignments: false\n",
      "      compute_timestamps: false\n",
      "      return_best_hypothesis: true\n",
      "      beam_alpha: 1.0\n",
      "      beam_beta: 0.0\n",
      "      kenlm_path: null\n",
      "      flashlight_cfg:\n",
      "        lexicon_path: null\n",
      "        boost_path: null\n",
      "        beam_size_token: 16\n",
      "        beam_threshold: 20.0\n",
      "        unk_weight: -.inf\n",
      "        sil_weight: 0.0\n",
      "      pyctcdecode_cfg:\n",
      "        beam_prune_logp: -10.0\n",
      "        token_min_logp: -5.0\n",
      "        prune_history: false\n",
      "        hotwords: null\n",
      "        hotword_weight: 10.0\n",
      "    wfst:\n",
      "      beam_size: 4\n",
      "      search_type: riva\n",
      "      return_best_hypothesis: true\n",
      "      preserve_alignments: false\n",
      "      compute_timestamps: false\n",
      "      decoding_mode: nbest\n",
      "      open_vocabulary_decoding: false\n",
      "      beam_width: 10.0\n",
      "      lm_weight: 1.0\n",
      "      device: cuda\n",
      "      arpa_lm_path: null\n",
      "      wfst_lm_path: null\n",
      "      riva_decoding_cfg: {}\n",
      "      k2_decoding_cfg:\n",
      "        search_beam: 20.0\n",
      "        output_beam: 10.0\n",
      "        min_active_states: 30\n",
      "        max_active_states: 10000\n",
      "    confidence_cfg:\n",
      "      preserve_frame_confidence: false\n",
      "      preserve_token_confidence: false\n",
      "      preserve_word_confidence: false\n",
      "      exclude_blank: true\n",
      "      aggregation: min\n",
      "      tdt_include_duration: false\n",
      "      method_cfg:\n",
      "        name: entropy\n",
      "        entropy_type: tsallis\n",
      "        alpha: 0.33\n",
      "        entropy_norm: exp\n",
      "        temperature: DEPRECATED\n",
      "    temperature: 1.0\n",
      "    \n",
      "[NeMo I 2025-05-07 17:58:49 align:250] Flag use_local_attention is set to True => will try to use local attention for model if it allows it\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Transcribing:   0%|          | 0/1 [00:00<?, ?it/s][NeMo W 2025-05-07 17:59:55 ctc_greedy_decoding:168] CTC decoding strategy 'greedy' is slower than 'greedy_batch', which implements the same exact interface. Consider changing your strategy to 'greedy_batch' for a free performance improvement.\n",
      "Transcribing: 100%|██████████| 1/1 [01:05<00:00, 65.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-05-07 17:59:55 data_prep:828] Calculated that the model downsample factor is 8 and therefore the ASR model output timestep duration is 0.08 -- will use this for all batches\n",
      "Running alignment for: /hdd2/Aman/scripts/ML_assign/Dataset/manifest/case16_2024_06_02_manifest.json\n",
      "[NeMo I 2025-05-07 18:02:01 align:159] Hydra config: pretrained_name: stt_en_fastconformer_hybrid_large_pc\n",
      "    model_path: null\n",
      "    manifest_filepath: /hdd2/Aman/scripts/ML_assign/Dataset/manifest/case16_2024_06_02_manifest.json\n",
      "    output_dir: /hdd2/Aman/scripts/ML_assign/src/nfa_output/case16_2024_06_02_manifest\n",
      "    align_using_pred_text: false\n",
      "    transcribe_device: cpu\n",
      "    viterbi_device: cpu\n",
      "    batch_size: 1\n",
      "    use_local_attention: true\n",
      "    additional_segment_grouping_separator: '|'\n",
      "    audio_filepath_parts_in_utt_id: 1\n",
      "    use_buffered_chunked_streaming: false\n",
      "    chunk_len_in_secs: 1.6\n",
      "    total_buffer_in_secs: 4.0\n",
      "    chunk_batch_size: 32\n",
      "    simulate_cache_aware_streaming: false\n",
      "    save_output_file_formats:\n",
      "    - ctm\n",
      "    - ass\n",
      "    ctm_file_config:\n",
      "      remove_blank_tokens: false\n",
      "      minimum_timestamp_duration: 0.0\n",
      "    ass_file_config:\n",
      "      fontsize: 20\n",
      "      vertical_alignment: center\n",
      "      resegment_text_to_fill_space: false\n",
      "      max_lines_per_segment: 2\n",
      "      text_already_spoken_rgb:\n",
      "      - 49\n",
      "      - 46\n",
      "      - 61\n",
      "      text_being_spoken_rgb:\n",
      "      - 57\n",
      "      - 171\n",
      "      - 9\n",
      "      text_not_yet_spoken_rgb:\n",
      "      - 194\n",
      "      - 193\n",
      "      - 199\n",
      "    \n",
      "[NeMo I 2025-05-07 18:02:01 align:228] Device to be used for transcription step (`transcribe_device`) is cpu\n",
      "[NeMo I 2025-05-07 18:02:01 align:234] Device to be used for viterbi step (`viterbi_device`) is cpu\n",
      "[NeMo I 2025-05-07 18:02:01 cloud:58] Found existing object /home/antpc/.cache/torch/NeMo/NeMo_2.4.0rc0/stt_en_fastconformer_hybrid_large_pc/465b32000fc320f5905fda11a1866ef6/stt_en_fastconformer_hybrid_large_pc.nemo.\n",
      "[NeMo I 2025-05-07 18:02:01 cloud:64] Re-using file from: /home/antpc/.cache/torch/NeMo/NeMo_2.4.0rc0/stt_en_fastconformer_hybrid_large_pc/465b32000fc320f5905fda11a1866ef6/stt_en_fastconformer_hybrid_large_pc.nemo\n",
      "[NeMo I 2025-05-07 18:02:01 common:827] Instantiating model from pre-trained checkpoint\n",
      "[NeMo I 2025-05-07 18:02:02 mixins:181] Tokenizer SentencePieceTokenizer initialized with 1024 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2025-05-07 18:02:03 modelPT:180] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
      "    Train config : \n",
      "    is_concat: false\n",
      "    concat_sampling_technique: random\n",
      "    concat_sampling_temperature: 8\n",
      "    concat_shuffle: true\n",
      "    concat_sampling_scale: 1.0\n",
      "    manifest_filepath:\n",
      "    - - /data/mml/en/tarred_train/pcstrip_sharded_manifests/manifest__OP_0..511_CL_.json\n",
      "    tarred_audio_filepaths:\n",
      "    - - /data/mml/en/tarred_train/audio__OP_0..511_CL_.tar\n",
      "    concat_sampling_probabilities:\n",
      "    - 1.0\n",
      "    sample_rate: 16000\n",
      "    batch_size: 16\n",
      "    shuffle: true\n",
      "    num_workers: 8\n",
      "    pin_memory: true\n",
      "    use_start_end_token: false\n",
      "    trim_silence: false\n",
      "    max_duration: 20\n",
      "    min_duration: 0.1\n",
      "    is_tarred: true\n",
      "    shard_manifests: true\n",
      "    defer_setup: true\n",
      "    shuffle_n: 528387\n",
      "    bucketing_strategy: synced_randomized\n",
      "    bucketing_batch_size: null\n",
      "    \n",
      "[NeMo W 2025-05-07 18:02:03 modelPT:187] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
      "    Validation config : \n",
      "    ds_item:\n",
      "    - name: en_\n",
      "      is_concat: true\n",
      "      concat_sampling_technique: random\n",
      "      concat_sampling_seed: 1234\n",
      "      concat_shuffle: false\n",
      "      concat_sampling_scale: 0.25\n",
      "      concat_sampling_probabilities:\n",
      "      - 0.088929287\n",
      "      - 0.116404906\n",
      "      - 0.101879436\n",
      "      - 0.171855007\n",
      "      - 0.160962192\n",
      "      - 0.092674082\n",
      "      - 0.041152255\n",
      "      - 0.226142835\n",
      "      manifest_filepath:\n",
      "      - - /data/mml/en/val_test/fisher/audio_manifest_dev_clean_pcstrip_en.json\n",
      "        - /data/mml/en/val_test/europarl/audio_manifest_dev_clean_pcstrip_en.json\n",
      "        - /data/mml/en/val_test/librispeech/audio_manifest_dev_clean_pcstrip_en.json\n",
      "        - /data/mml/en/val_test/mcv11/audio_manifest_dev_clean_pcstrip_en.json\n",
      "        - /data/mml/en/val_test/nsc1/audio_manifest_dev_clean_pcstrip_en.json\n",
      "        - /data/mml/en/val_test/SPGI/audio_manifest_dev_clean_pcstrip_en.json\n",
      "        - /data/mml/en/val_test/voxpopuli/audio_manifest_dev_clean_pcstrip_en.json\n",
      "        - /data/mml/en/val_test/mls_dev/mls_dev_pcstrip_en.json\n",
      "      sample_rate: 16000\n",
      "      batch_size: 64\n",
      "      shuffle: false\n",
      "      num_workers: 1\n",
      "      pin_memory: true\n",
      "      max_duration: 20.0\n",
      "      min_duration: 0.1\n",
      "      use_start_end_token: false\n",
      "    - name: fisher_\n",
      "      is_concat: false\n",
      "      manifest_filepath: /data/mml/en/val_test/fisher/audio_manifest_dev_clean_pcstrip_en.json\n",
      "      sample_rate: 16000\n",
      "      batch_size: 64\n",
      "      shuffle: false\n",
      "      num_workers: 1\n",
      "      pin_memory: true\n",
      "      max_duration: 20.0\n",
      "      min_duration: 0.1\n",
      "      use_start_end_token: false\n",
      "    - name: europarl_\n",
      "      is_concat: false\n",
      "      manifest_filepath: /data/mml/en/val_test/europarl/audio_manifest_dev_clean_pcstrip_en.json\n",
      "      sample_rate: 16000\n",
      "      batch_size: 64\n",
      "      shuffle: false\n",
      "      num_workers: 1\n",
      "      pin_memory: true\n",
      "      max_duration: 20.0\n",
      "      min_duration: 0.1\n",
      "      use_start_end_token: false\n",
      "    - name: librispeech_\n",
      "      is_concat: false\n",
      "      manifest_filepath: /data/mml/en/val_test/librispeech/audio_manifest_dev_clean_pcstrip_en.json\n",
      "      sample_rate: 16000\n",
      "      batch_size: 64\n",
      "      shuffle: false\n",
      "      num_workers: 1\n",
      "      pin_memory: true\n",
      "      max_duration: 20.0\n",
      "      min_duration: 0.1\n",
      "      use_start_end_token: false\n",
      "    - name: mcv11_\n",
      "      is_concat: false\n",
      "      manifest_filepath: /data/mml/en/val_test/mcv11/audio_manifest_dev_clean_pcstrip_en.json\n",
      "      sample_rate: 16000\n",
      "      batch_size: 64\n",
      "      shuffle: false\n",
      "      num_workers: 1\n",
      "      pin_memory: true\n",
      "      max_duration: 20.0\n",
      "      min_duration: 0.1\n",
      "      use_start_end_token: false\n",
      "    - name: nsc1_\n",
      "      is_concat: false\n",
      "      manifest_filepath: /data/mml/en/val_test/nsc1/audio_manifest_dev_clean_pcstrip_en.json\n",
      "      sample_rate: 16000\n",
      "      batch_size: 64\n",
      "      shuffle: false\n",
      "      num_workers: 1\n",
      "      pin_memory: true\n",
      "      max_duration: 20.0\n",
      "      min_duration: 0.1\n",
      "      use_start_end_token: false\n",
      "    - name: spgi_\n",
      "      is_concat: false\n",
      "      manifest_filepath: /data/mml/en/val_test/SPGI/audio_manifest_dev_clean_pcstrip_en.json\n",
      "      sample_rate: 16000\n",
      "      batch_size: 64\n",
      "      shuffle: false\n",
      "      num_workers: 1\n",
      "      pin_memory: true\n",
      "      max_duration: 20.0\n",
      "      min_duration: 0.1\n",
      "      use_start_end_token: false\n",
      "    - name: voxpopuli_\n",
      "      is_concat: false\n",
      "      manifest_filepath: /data/mml/en/val_test/voxpopuli/audio_manifest_dev_clean_pcstrip_en.json\n",
      "      sample_rate: 16000\n",
      "      batch_size: 64\n",
      "      shuffle: false\n",
      "      num_workers: 1\n",
      "      pin_memory: true\n",
      "      max_duration: 20.0\n",
      "      min_duration: 0.1\n",
      "      use_start_end_token: false\n",
      "    - name: mls_\n",
      "      is_concat: false\n",
      "      manifest_filepath: /data/mml/en/val_test/mls_dev/mls_dev_pcstrip_en.json\n",
      "      sample_rate: 16000\n",
      "      batch_size: 64\n",
      "      shuffle: false\n",
      "      num_workers: 1\n",
      "      pin_memory: true\n",
      "      max_duration: 20.0\n",
      "      min_duration: 0.1\n",
      "      use_start_end_token: false\n",
      "    max_duration: 20\n",
      "    \n",
      "[NeMo W 2025-05-07 18:02:03 modelPT:194] Please call the ModelPT.setup_test_data() or ModelPT.setup_multiple_test_data() method and provide a valid configuration file to setup the test data loader(s).\n",
      "    Test config : \n",
      "    manifest_filepath: null\n",
      "    sample_rate: 16000\n",
      "    batch_size: 16\n",
      "    shuffle: false\n",
      "    num_workers: 8\n",
      "    pin_memory: true\n",
      "    use_start_end_token: false\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-05-07 18:02:03 features:305] PADDING: 0\n",
      "[NeMo I 2025-05-07 18:02:05 rnnt_models:226] Using RNNT Loss : warprnnt_numba\n",
      "    Loss warprnnt_numba_kwargs: {'fastemit_lambda': 0.0, 'clamp': -1.0}\n",
      "[NeMo I 2025-05-07 18:02:05 rnnt_models:226] Using RNNT Loss : warprnnt_numba\n",
      "    Loss warprnnt_numba_kwargs: {'fastemit_lambda': 0.0, 'clamp': -1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2025-05-07 18:02:05 rnnt_loop_labels_computer:290] No conditional node support for Cuda.\n",
      "    Cuda graphs with while loops are disabled, decoding speed will be slower\n",
      "    Reason: Driver supports cuda toolkit version 12.2, but the driver needs to support at least 12,3. Please update your cuda driver.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-05-07 18:02:05 rnnt_models:226] Using RNNT Loss : warprnnt_numba\n",
      "    Loss warprnnt_numba_kwargs: {'fastemit_lambda': 0.0, 'clamp': -1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2025-05-07 18:02:05 rnnt_loop_labels_computer:290] No conditional node support for Cuda.\n",
      "    Cuda graphs with while loops are disabled, decoding speed will be slower\n",
      "    Reason: Driver supports cuda toolkit version 12.2, but the driver needs to support at least 12,3. Please update your cuda driver.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-05-07 18:02:06 save_restore_connector:275] Model EncDecHybridRNNTCTCBPEModel was successfully restored from /home/antpc/.cache/torch/NeMo/NeMo_2.4.0rc0/stt_en_fastconformer_hybrid_large_pc/465b32000fc320f5905fda11a1866ef6/stt_en_fastconformer_hybrid_large_pc.nemo.\n",
      "[NeMo I 2025-05-07 18:02:06 hybrid_rnnt_ctc_bpe_models:487] No `decoding_cfg` passed when changing decoding strategy, using internal config\n",
      "[NeMo I 2025-05-07 18:02:06 hybrid_rnnt_ctc_bpe_models:512] Changed decoding strategy of the CTC decoder to \n",
      "    strategy: greedy\n",
      "    preserve_alignments: null\n",
      "    compute_timestamps: null\n",
      "    word_seperator: ' '\n",
      "    segment_seperators:\n",
      "    - .\n",
      "    - '!'\n",
      "    - '?'\n",
      "    segment_gap_threshold: null\n",
      "    ctc_timestamp_type: all\n",
      "    batch_dim_index: 0\n",
      "    greedy:\n",
      "      preserve_alignments: false\n",
      "      compute_timestamps: false\n",
      "      preserve_frame_confidence: false\n",
      "      confidence_method_cfg:\n",
      "        name: entropy\n",
      "        entropy_type: tsallis\n",
      "        alpha: 0.33\n",
      "        entropy_norm: exp\n",
      "        temperature: DEPRECATED\n",
      "    beam:\n",
      "      beam_size: 4\n",
      "      search_type: default\n",
      "      preserve_alignments: false\n",
      "      compute_timestamps: false\n",
      "      return_best_hypothesis: true\n",
      "      beam_alpha: 1.0\n",
      "      beam_beta: 0.0\n",
      "      kenlm_path: null\n",
      "      flashlight_cfg:\n",
      "        lexicon_path: null\n",
      "        boost_path: null\n",
      "        beam_size_token: 16\n",
      "        beam_threshold: 20.0\n",
      "        unk_weight: -.inf\n",
      "        sil_weight: 0.0\n",
      "      pyctcdecode_cfg:\n",
      "        beam_prune_logp: -10.0\n",
      "        token_min_logp: -5.0\n",
      "        prune_history: false\n",
      "        hotwords: null\n",
      "        hotword_weight: 10.0\n",
      "    wfst:\n",
      "      beam_size: 4\n",
      "      search_type: riva\n",
      "      return_best_hypothesis: true\n",
      "      preserve_alignments: false\n",
      "      compute_timestamps: false\n",
      "      decoding_mode: nbest\n",
      "      open_vocabulary_decoding: false\n",
      "      beam_width: 10.0\n",
      "      lm_weight: 1.0\n",
      "      device: cuda\n",
      "      arpa_lm_path: null\n",
      "      wfst_lm_path: null\n",
      "      riva_decoding_cfg: {}\n",
      "      k2_decoding_cfg:\n",
      "        search_beam: 20.0\n",
      "        output_beam: 10.0\n",
      "        min_active_states: 30\n",
      "        max_active_states: 10000\n",
      "    confidence_cfg:\n",
      "      preserve_frame_confidence: false\n",
      "      preserve_token_confidence: false\n",
      "      preserve_word_confidence: false\n",
      "      exclude_blank: true\n",
      "      aggregation: min\n",
      "      tdt_include_duration: false\n",
      "      method_cfg:\n",
      "        name: entropy\n",
      "        entropy_type: tsallis\n",
      "        alpha: 0.33\n",
      "        entropy_norm: exp\n",
      "        temperature: DEPRECATED\n",
      "    temperature: 1.0\n",
      "    \n",
      "[NeMo I 2025-05-07 18:02:06 align:250] Flag use_local_attention is set to True => will try to use local attention for model if it allows it\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Transcribing:   0%|          | 0/1 [00:00<?, ?it/s][NeMo W 2025-05-07 18:03:50 ctc_greedy_decoding:168] CTC decoding strategy 'greedy' is slower than 'greedy_batch', which implements the same exact interface. Consider changing your strategy to 'greedy_batch' for a free performance improvement.\n",
      "Transcribing: 100%|██████████| 1/1 [01:44<00:00, 104.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-05-07 18:03:51 data_prep:828] Calculated that the model downsample factor is 8 and therefore the ASR model output timestep duration is 0.08 -- will use this for all batches\n",
      "Running alignment for: /hdd2/Aman/scripts/ML_assign/Dataset/manifest/case12_2023_12_10_manifest.json\n",
      "[NeMo I 2025-05-07 18:07:17 align:159] Hydra config: pretrained_name: stt_en_fastconformer_hybrid_large_pc\n",
      "    model_path: null\n",
      "    manifest_filepath: /hdd2/Aman/scripts/ML_assign/Dataset/manifest/case12_2023_12_10_manifest.json\n",
      "    output_dir: /hdd2/Aman/scripts/ML_assign/src/nfa_output/case12_2023_12_10_manifest\n",
      "    align_using_pred_text: false\n",
      "    transcribe_device: cpu\n",
      "    viterbi_device: cpu\n",
      "    batch_size: 1\n",
      "    use_local_attention: true\n",
      "    additional_segment_grouping_separator: '|'\n",
      "    audio_filepath_parts_in_utt_id: 1\n",
      "    use_buffered_chunked_streaming: false\n",
      "    chunk_len_in_secs: 1.6\n",
      "    total_buffer_in_secs: 4.0\n",
      "    chunk_batch_size: 32\n",
      "    simulate_cache_aware_streaming: false\n",
      "    save_output_file_formats:\n",
      "    - ctm\n",
      "    - ass\n",
      "    ctm_file_config:\n",
      "      remove_blank_tokens: false\n",
      "      minimum_timestamp_duration: 0.0\n",
      "    ass_file_config:\n",
      "      fontsize: 20\n",
      "      vertical_alignment: center\n",
      "      resegment_text_to_fill_space: false\n",
      "      max_lines_per_segment: 2\n",
      "      text_already_spoken_rgb:\n",
      "      - 49\n",
      "      - 46\n",
      "      - 61\n",
      "      text_being_spoken_rgb:\n",
      "      - 57\n",
      "      - 171\n",
      "      - 9\n",
      "      text_not_yet_spoken_rgb:\n",
      "      - 194\n",
      "      - 193\n",
      "      - 199\n",
      "    \n",
      "[NeMo I 2025-05-07 18:07:17 align:228] Device to be used for transcription step (`transcribe_device`) is cpu\n",
      "[NeMo I 2025-05-07 18:07:17 align:234] Device to be used for viterbi step (`viterbi_device`) is cpu\n",
      "[NeMo I 2025-05-07 18:07:17 cloud:58] Found existing object /home/antpc/.cache/torch/NeMo/NeMo_2.4.0rc0/stt_en_fastconformer_hybrid_large_pc/465b32000fc320f5905fda11a1866ef6/stt_en_fastconformer_hybrid_large_pc.nemo.\n",
      "[NeMo I 2025-05-07 18:07:17 cloud:64] Re-using file from: /home/antpc/.cache/torch/NeMo/NeMo_2.4.0rc0/stt_en_fastconformer_hybrid_large_pc/465b32000fc320f5905fda11a1866ef6/stt_en_fastconformer_hybrid_large_pc.nemo\n",
      "[NeMo I 2025-05-07 18:07:17 common:827] Instantiating model from pre-trained checkpoint\n",
      "[NeMo I 2025-05-07 18:07:19 mixins:181] Tokenizer SentencePieceTokenizer initialized with 1024 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2025-05-07 18:07:20 modelPT:180] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
      "    Train config : \n",
      "    is_concat: false\n",
      "    concat_sampling_technique: random\n",
      "    concat_sampling_temperature: 8\n",
      "    concat_shuffle: true\n",
      "    concat_sampling_scale: 1.0\n",
      "    manifest_filepath:\n",
      "    - - /data/mml/en/tarred_train/pcstrip_sharded_manifests/manifest__OP_0..511_CL_.json\n",
      "    tarred_audio_filepaths:\n",
      "    - - /data/mml/en/tarred_train/audio__OP_0..511_CL_.tar\n",
      "    concat_sampling_probabilities:\n",
      "    - 1.0\n",
      "    sample_rate: 16000\n",
      "    batch_size: 16\n",
      "    shuffle: true\n",
      "    num_workers: 8\n",
      "    pin_memory: true\n",
      "    use_start_end_token: false\n",
      "    trim_silence: false\n",
      "    max_duration: 20\n",
      "    min_duration: 0.1\n",
      "    is_tarred: true\n",
      "    shard_manifests: true\n",
      "    defer_setup: true\n",
      "    shuffle_n: 528387\n",
      "    bucketing_strategy: synced_randomized\n",
      "    bucketing_batch_size: null\n",
      "    \n",
      "[NeMo W 2025-05-07 18:07:20 modelPT:187] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
      "    Validation config : \n",
      "    ds_item:\n",
      "    - name: en_\n",
      "      is_concat: true\n",
      "      concat_sampling_technique: random\n",
      "      concat_sampling_seed: 1234\n",
      "      concat_shuffle: false\n",
      "      concat_sampling_scale: 0.25\n",
      "      concat_sampling_probabilities:\n",
      "      - 0.088929287\n",
      "      - 0.116404906\n",
      "      - 0.101879436\n",
      "      - 0.171855007\n",
      "      - 0.160962192\n",
      "      - 0.092674082\n",
      "      - 0.041152255\n",
      "      - 0.226142835\n",
      "      manifest_filepath:\n",
      "      - - /data/mml/en/val_test/fisher/audio_manifest_dev_clean_pcstrip_en.json\n",
      "        - /data/mml/en/val_test/europarl/audio_manifest_dev_clean_pcstrip_en.json\n",
      "        - /data/mml/en/val_test/librispeech/audio_manifest_dev_clean_pcstrip_en.json\n",
      "        - /data/mml/en/val_test/mcv11/audio_manifest_dev_clean_pcstrip_en.json\n",
      "        - /data/mml/en/val_test/nsc1/audio_manifest_dev_clean_pcstrip_en.json\n",
      "        - /data/mml/en/val_test/SPGI/audio_manifest_dev_clean_pcstrip_en.json\n",
      "        - /data/mml/en/val_test/voxpopuli/audio_manifest_dev_clean_pcstrip_en.json\n",
      "        - /data/mml/en/val_test/mls_dev/mls_dev_pcstrip_en.json\n",
      "      sample_rate: 16000\n",
      "      batch_size: 64\n",
      "      shuffle: false\n",
      "      num_workers: 1\n",
      "      pin_memory: true\n",
      "      max_duration: 20.0\n",
      "      min_duration: 0.1\n",
      "      use_start_end_token: false\n",
      "    - name: fisher_\n",
      "      is_concat: false\n",
      "      manifest_filepath: /data/mml/en/val_test/fisher/audio_manifest_dev_clean_pcstrip_en.json\n",
      "      sample_rate: 16000\n",
      "      batch_size: 64\n",
      "      shuffle: false\n",
      "      num_workers: 1\n",
      "      pin_memory: true\n",
      "      max_duration: 20.0\n",
      "      min_duration: 0.1\n",
      "      use_start_end_token: false\n",
      "    - name: europarl_\n",
      "      is_concat: false\n",
      "      manifest_filepath: /data/mml/en/val_test/europarl/audio_manifest_dev_clean_pcstrip_en.json\n",
      "      sample_rate: 16000\n",
      "      batch_size: 64\n",
      "      shuffle: false\n",
      "      num_workers: 1\n",
      "      pin_memory: true\n",
      "      max_duration: 20.0\n",
      "      min_duration: 0.1\n",
      "      use_start_end_token: false\n",
      "    - name: librispeech_\n",
      "      is_concat: false\n",
      "      manifest_filepath: /data/mml/en/val_test/librispeech/audio_manifest_dev_clean_pcstrip_en.json\n",
      "      sample_rate: 16000\n",
      "      batch_size: 64\n",
      "      shuffle: false\n",
      "      num_workers: 1\n",
      "      pin_memory: true\n",
      "      max_duration: 20.0\n",
      "      min_duration: 0.1\n",
      "      use_start_end_token: false\n",
      "    - name: mcv11_\n",
      "      is_concat: false\n",
      "      manifest_filepath: /data/mml/en/val_test/mcv11/audio_manifest_dev_clean_pcstrip_en.json\n",
      "      sample_rate: 16000\n",
      "      batch_size: 64\n",
      "      shuffle: false\n",
      "      num_workers: 1\n",
      "      pin_memory: true\n",
      "      max_duration: 20.0\n",
      "      min_duration: 0.1\n",
      "      use_start_end_token: false\n",
      "    - name: nsc1_\n",
      "      is_concat: false\n",
      "      manifest_filepath: /data/mml/en/val_test/nsc1/audio_manifest_dev_clean_pcstrip_en.json\n",
      "      sample_rate: 16000\n",
      "      batch_size: 64\n",
      "      shuffle: false\n",
      "      num_workers: 1\n",
      "      pin_memory: true\n",
      "      max_duration: 20.0\n",
      "      min_duration: 0.1\n",
      "      use_start_end_token: false\n",
      "    - name: spgi_\n",
      "      is_concat: false\n",
      "      manifest_filepath: /data/mml/en/val_test/SPGI/audio_manifest_dev_clean_pcstrip_en.json\n",
      "      sample_rate: 16000\n",
      "      batch_size: 64\n",
      "      shuffle: false\n",
      "      num_workers: 1\n",
      "      pin_memory: true\n",
      "      max_duration: 20.0\n",
      "      min_duration: 0.1\n",
      "      use_start_end_token: false\n",
      "    - name: voxpopuli_\n",
      "      is_concat: false\n",
      "      manifest_filepath: /data/mml/en/val_test/voxpopuli/audio_manifest_dev_clean_pcstrip_en.json\n",
      "      sample_rate: 16000\n",
      "      batch_size: 64\n",
      "      shuffle: false\n",
      "      num_workers: 1\n",
      "      pin_memory: true\n",
      "      max_duration: 20.0\n",
      "      min_duration: 0.1\n",
      "      use_start_end_token: false\n",
      "    - name: mls_\n",
      "      is_concat: false\n",
      "      manifest_filepath: /data/mml/en/val_test/mls_dev/mls_dev_pcstrip_en.json\n",
      "      sample_rate: 16000\n",
      "      batch_size: 64\n",
      "      shuffle: false\n",
      "      num_workers: 1\n",
      "      pin_memory: true\n",
      "      max_duration: 20.0\n",
      "      min_duration: 0.1\n",
      "      use_start_end_token: false\n",
      "    max_duration: 20\n",
      "    \n",
      "[NeMo W 2025-05-07 18:07:20 modelPT:194] Please call the ModelPT.setup_test_data() or ModelPT.setup_multiple_test_data() method and provide a valid configuration file to setup the test data loader(s).\n",
      "    Test config : \n",
      "    manifest_filepath: null\n",
      "    sample_rate: 16000\n",
      "    batch_size: 16\n",
      "    shuffle: false\n",
      "    num_workers: 8\n",
      "    pin_memory: true\n",
      "    use_start_end_token: false\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-05-07 18:07:20 features:305] PADDING: 0\n",
      "[NeMo I 2025-05-07 18:07:21 rnnt_models:226] Using RNNT Loss : warprnnt_numba\n",
      "    Loss warprnnt_numba_kwargs: {'fastemit_lambda': 0.0, 'clamp': -1.0}\n",
      "[NeMo I 2025-05-07 18:07:21 rnnt_models:226] Using RNNT Loss : warprnnt_numba\n",
      "    Loss warprnnt_numba_kwargs: {'fastemit_lambda': 0.0, 'clamp': -1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2025-05-07 18:07:22 rnnt_loop_labels_computer:290] No conditional node support for Cuda.\n",
      "    Cuda graphs with while loops are disabled, decoding speed will be slower\n",
      "    Reason: Driver supports cuda toolkit version 12.2, but the driver needs to support at least 12,3. Please update your cuda driver.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-05-07 18:07:22 rnnt_models:226] Using RNNT Loss : warprnnt_numba\n",
      "    Loss warprnnt_numba_kwargs: {'fastemit_lambda': 0.0, 'clamp': -1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2025-05-07 18:07:22 rnnt_loop_labels_computer:290] No conditional node support for Cuda.\n",
      "    Cuda graphs with while loops are disabled, decoding speed will be slower\n",
      "    Reason: Driver supports cuda toolkit version 12.2, but the driver needs to support at least 12,3. Please update your cuda driver.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-05-07 18:07:22 save_restore_connector:275] Model EncDecHybridRNNTCTCBPEModel was successfully restored from /home/antpc/.cache/torch/NeMo/NeMo_2.4.0rc0/stt_en_fastconformer_hybrid_large_pc/465b32000fc320f5905fda11a1866ef6/stt_en_fastconformer_hybrid_large_pc.nemo.\n",
      "[NeMo I 2025-05-07 18:07:22 hybrid_rnnt_ctc_bpe_models:487] No `decoding_cfg` passed when changing decoding strategy, using internal config\n",
      "[NeMo I 2025-05-07 18:07:22 hybrid_rnnt_ctc_bpe_models:512] Changed decoding strategy of the CTC decoder to \n",
      "    strategy: greedy\n",
      "    preserve_alignments: null\n",
      "    compute_timestamps: null\n",
      "    word_seperator: ' '\n",
      "    segment_seperators:\n",
      "    - .\n",
      "    - '!'\n",
      "    - '?'\n",
      "    segment_gap_threshold: null\n",
      "    ctc_timestamp_type: all\n",
      "    batch_dim_index: 0\n",
      "    greedy:\n",
      "      preserve_alignments: false\n",
      "      compute_timestamps: false\n",
      "      preserve_frame_confidence: false\n",
      "      confidence_method_cfg:\n",
      "        name: entropy\n",
      "        entropy_type: tsallis\n",
      "        alpha: 0.33\n",
      "        entropy_norm: exp\n",
      "        temperature: DEPRECATED\n",
      "    beam:\n",
      "      beam_size: 4\n",
      "      search_type: default\n",
      "      preserve_alignments: false\n",
      "      compute_timestamps: false\n",
      "      return_best_hypothesis: true\n",
      "      beam_alpha: 1.0\n",
      "      beam_beta: 0.0\n",
      "      kenlm_path: null\n",
      "      flashlight_cfg:\n",
      "        lexicon_path: null\n",
      "        boost_path: null\n",
      "        beam_size_token: 16\n",
      "        beam_threshold: 20.0\n",
      "        unk_weight: -.inf\n",
      "        sil_weight: 0.0\n",
      "      pyctcdecode_cfg:\n",
      "        beam_prune_logp: -10.0\n",
      "        token_min_logp: -5.0\n",
      "        prune_history: false\n",
      "        hotwords: null\n",
      "        hotword_weight: 10.0\n",
      "    wfst:\n",
      "      beam_size: 4\n",
      "      search_type: riva\n",
      "      return_best_hypothesis: true\n",
      "      preserve_alignments: false\n",
      "      compute_timestamps: false\n",
      "      decoding_mode: nbest\n",
      "      open_vocabulary_decoding: false\n",
      "      beam_width: 10.0\n",
      "      lm_weight: 1.0\n",
      "      device: cuda\n",
      "      arpa_lm_path: null\n",
      "      wfst_lm_path: null\n",
      "      riva_decoding_cfg: {}\n",
      "      k2_decoding_cfg:\n",
      "        search_beam: 20.0\n",
      "        output_beam: 10.0\n",
      "        min_active_states: 30\n",
      "        max_active_states: 10000\n",
      "    confidence_cfg:\n",
      "      preserve_frame_confidence: false\n",
      "      preserve_token_confidence: false\n",
      "      preserve_word_confidence: false\n",
      "      exclude_blank: true\n",
      "      aggregation: min\n",
      "      tdt_include_duration: false\n",
      "      method_cfg:\n",
      "        name: entropy\n",
      "        entropy_type: tsallis\n",
      "        alpha: 0.33\n",
      "        entropy_norm: exp\n",
      "        temperature: DEPRECATED\n",
      "    temperature: 1.0\n",
      "    \n",
      "[NeMo I 2025-05-07 18:07:22 align:250] Flag use_local_attention is set to True => will try to use local attention for model if it allows it\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Transcribing:   0%|          | 0/1 [00:00<?, ?it/s][NeMo W 2025-05-07 18:08:51 ctc_greedy_decoding:168] CTC decoding strategy 'greedy' is slower than 'greedy_batch', which implements the same exact interface. Consider changing your strategy to 'greedy_batch' for a free performance improvement.\n",
      "Transcribing: 100%|██████████| 1/1 [01:27<00:00, 87.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-05-07 18:08:52 data_prep:828] Calculated that the model downsample factor is 8 and therefore the ASR model output timestep duration is 0.08 -- will use this for all batches\n",
      "Running alignment for: /hdd2/Aman/scripts/ML_assign/Dataset/manifest/case6_2023_04_10_manifest.json\n",
      "[NeMo I 2025-05-07 18:11:49 align:159] Hydra config: pretrained_name: stt_en_fastconformer_hybrid_large_pc\n",
      "    model_path: null\n",
      "    manifest_filepath: /hdd2/Aman/scripts/ML_assign/Dataset/manifest/case6_2023_04_10_manifest.json\n",
      "    output_dir: /hdd2/Aman/scripts/ML_assign/src/nfa_output/case6_2023_04_10_manifest\n",
      "    align_using_pred_text: false\n",
      "    transcribe_device: cpu\n",
      "    viterbi_device: cpu\n",
      "    batch_size: 1\n",
      "    use_local_attention: true\n",
      "    additional_segment_grouping_separator: '|'\n",
      "    audio_filepath_parts_in_utt_id: 1\n",
      "    use_buffered_chunked_streaming: false\n",
      "    chunk_len_in_secs: 1.6\n",
      "    total_buffer_in_secs: 4.0\n",
      "    chunk_batch_size: 32\n",
      "    simulate_cache_aware_streaming: false\n",
      "    save_output_file_formats:\n",
      "    - ctm\n",
      "    - ass\n",
      "    ctm_file_config:\n",
      "      remove_blank_tokens: false\n",
      "      minimum_timestamp_duration: 0.0\n",
      "    ass_file_config:\n",
      "      fontsize: 20\n",
      "      vertical_alignment: center\n",
      "      resegment_text_to_fill_space: false\n",
      "      max_lines_per_segment: 2\n",
      "      text_already_spoken_rgb:\n",
      "      - 49\n",
      "      - 46\n",
      "      - 61\n",
      "      text_being_spoken_rgb:\n",
      "      - 57\n",
      "      - 171\n",
      "      - 9\n",
      "      text_not_yet_spoken_rgb:\n",
      "      - 194\n",
      "      - 193\n",
      "      - 199\n",
      "    \n",
      "[NeMo I 2025-05-07 18:11:49 align:228] Device to be used for transcription step (`transcribe_device`) is cpu\n",
      "[NeMo I 2025-05-07 18:11:49 align:234] Device to be used for viterbi step (`viterbi_device`) is cpu\n",
      "[NeMo I 2025-05-07 18:11:49 cloud:58] Found existing object /home/antpc/.cache/torch/NeMo/NeMo_2.4.0rc0/stt_en_fastconformer_hybrid_large_pc/465b32000fc320f5905fda11a1866ef6/stt_en_fastconformer_hybrid_large_pc.nemo.\n",
      "[NeMo I 2025-05-07 18:11:49 cloud:64] Re-using file from: /home/antpc/.cache/torch/NeMo/NeMo_2.4.0rc0/stt_en_fastconformer_hybrid_large_pc/465b32000fc320f5905fda11a1866ef6/stt_en_fastconformer_hybrid_large_pc.nemo\n",
      "[NeMo I 2025-05-07 18:11:49 common:827] Instantiating model from pre-trained checkpoint\n",
      "[NeMo I 2025-05-07 18:11:50 mixins:181] Tokenizer SentencePieceTokenizer initialized with 1024 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2025-05-07 18:11:52 modelPT:180] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
      "    Train config : \n",
      "    is_concat: false\n",
      "    concat_sampling_technique: random\n",
      "    concat_sampling_temperature: 8\n",
      "    concat_shuffle: true\n",
      "    concat_sampling_scale: 1.0\n",
      "    manifest_filepath:\n",
      "    - - /data/mml/en/tarred_train/pcstrip_sharded_manifests/manifest__OP_0..511_CL_.json\n",
      "    tarred_audio_filepaths:\n",
      "    - - /data/mml/en/tarred_train/audio__OP_0..511_CL_.tar\n",
      "    concat_sampling_probabilities:\n",
      "    - 1.0\n",
      "    sample_rate: 16000\n",
      "    batch_size: 16\n",
      "    shuffle: true\n",
      "    num_workers: 8\n",
      "    pin_memory: true\n",
      "    use_start_end_token: false\n",
      "    trim_silence: false\n",
      "    max_duration: 20\n",
      "    min_duration: 0.1\n",
      "    is_tarred: true\n",
      "    shard_manifests: true\n",
      "    defer_setup: true\n",
      "    shuffle_n: 528387\n",
      "    bucketing_strategy: synced_randomized\n",
      "    bucketing_batch_size: null\n",
      "    \n",
      "[NeMo W 2025-05-07 18:11:52 modelPT:187] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
      "    Validation config : \n",
      "    ds_item:\n",
      "    - name: en_\n",
      "      is_concat: true\n",
      "      concat_sampling_technique: random\n",
      "      concat_sampling_seed: 1234\n",
      "      concat_shuffle: false\n",
      "      concat_sampling_scale: 0.25\n",
      "      concat_sampling_probabilities:\n",
      "      - 0.088929287\n",
      "      - 0.116404906\n",
      "      - 0.101879436\n",
      "      - 0.171855007\n",
      "      - 0.160962192\n",
      "      - 0.092674082\n",
      "      - 0.041152255\n",
      "      - 0.226142835\n",
      "      manifest_filepath:\n",
      "      - - /data/mml/en/val_test/fisher/audio_manifest_dev_clean_pcstrip_en.json\n",
      "        - /data/mml/en/val_test/europarl/audio_manifest_dev_clean_pcstrip_en.json\n",
      "        - /data/mml/en/val_test/librispeech/audio_manifest_dev_clean_pcstrip_en.json\n",
      "        - /data/mml/en/val_test/mcv11/audio_manifest_dev_clean_pcstrip_en.json\n",
      "        - /data/mml/en/val_test/nsc1/audio_manifest_dev_clean_pcstrip_en.json\n",
      "        - /data/mml/en/val_test/SPGI/audio_manifest_dev_clean_pcstrip_en.json\n",
      "        - /data/mml/en/val_test/voxpopuli/audio_manifest_dev_clean_pcstrip_en.json\n",
      "        - /data/mml/en/val_test/mls_dev/mls_dev_pcstrip_en.json\n",
      "      sample_rate: 16000\n",
      "      batch_size: 64\n",
      "      shuffle: false\n",
      "      num_workers: 1\n",
      "      pin_memory: true\n",
      "      max_duration: 20.0\n",
      "      min_duration: 0.1\n",
      "      use_start_end_token: false\n",
      "    - name: fisher_\n",
      "      is_concat: false\n",
      "      manifest_filepath: /data/mml/en/val_test/fisher/audio_manifest_dev_clean_pcstrip_en.json\n",
      "      sample_rate: 16000\n",
      "      batch_size: 64\n",
      "      shuffle: false\n",
      "      num_workers: 1\n",
      "      pin_memory: true\n",
      "      max_duration: 20.0\n",
      "      min_duration: 0.1\n",
      "      use_start_end_token: false\n",
      "    - name: europarl_\n",
      "      is_concat: false\n",
      "      manifest_filepath: /data/mml/en/val_test/europarl/audio_manifest_dev_clean_pcstrip_en.json\n",
      "      sample_rate: 16000\n",
      "      batch_size: 64\n",
      "      shuffle: false\n",
      "      num_workers: 1\n",
      "      pin_memory: true\n",
      "      max_duration: 20.0\n",
      "      min_duration: 0.1\n",
      "      use_start_end_token: false\n",
      "    - name: librispeech_\n",
      "      is_concat: false\n",
      "      manifest_filepath: /data/mml/en/val_test/librispeech/audio_manifest_dev_clean_pcstrip_en.json\n",
      "      sample_rate: 16000\n",
      "      batch_size: 64\n",
      "      shuffle: false\n",
      "      num_workers: 1\n",
      "      pin_memory: true\n",
      "      max_duration: 20.0\n",
      "      min_duration: 0.1\n",
      "      use_start_end_token: false\n",
      "    - name: mcv11_\n",
      "      is_concat: false\n",
      "      manifest_filepath: /data/mml/en/val_test/mcv11/audio_manifest_dev_clean_pcstrip_en.json\n",
      "      sample_rate: 16000\n",
      "      batch_size: 64\n",
      "      shuffle: false\n",
      "      num_workers: 1\n",
      "      pin_memory: true\n",
      "      max_duration: 20.0\n",
      "      min_duration: 0.1\n",
      "      use_start_end_token: false\n",
      "    - name: nsc1_\n",
      "      is_concat: false\n",
      "      manifest_filepath: /data/mml/en/val_test/nsc1/audio_manifest_dev_clean_pcstrip_en.json\n",
      "      sample_rate: 16000\n",
      "      batch_size: 64\n",
      "      shuffle: false\n",
      "      num_workers: 1\n",
      "      pin_memory: true\n",
      "      max_duration: 20.0\n",
      "      min_duration: 0.1\n",
      "      use_start_end_token: false\n",
      "    - name: spgi_\n",
      "      is_concat: false\n",
      "      manifest_filepath: /data/mml/en/val_test/SPGI/audio_manifest_dev_clean_pcstrip_en.json\n",
      "      sample_rate: 16000\n",
      "      batch_size: 64\n",
      "      shuffle: false\n",
      "      num_workers: 1\n",
      "      pin_memory: true\n",
      "      max_duration: 20.0\n",
      "      min_duration: 0.1\n",
      "      use_start_end_token: false\n",
      "    - name: voxpopuli_\n",
      "      is_concat: false\n",
      "      manifest_filepath: /data/mml/en/val_test/voxpopuli/audio_manifest_dev_clean_pcstrip_en.json\n",
      "      sample_rate: 16000\n",
      "      batch_size: 64\n",
      "      shuffle: false\n",
      "      num_workers: 1\n",
      "      pin_memory: true\n",
      "      max_duration: 20.0\n",
      "      min_duration: 0.1\n",
      "      use_start_end_token: false\n",
      "    - name: mls_\n",
      "      is_concat: false\n",
      "      manifest_filepath: /data/mml/en/val_test/mls_dev/mls_dev_pcstrip_en.json\n",
      "      sample_rate: 16000\n",
      "      batch_size: 64\n",
      "      shuffle: false\n",
      "      num_workers: 1\n",
      "      pin_memory: true\n",
      "      max_duration: 20.0\n",
      "      min_duration: 0.1\n",
      "      use_start_end_token: false\n",
      "    max_duration: 20\n",
      "    \n",
      "[NeMo W 2025-05-07 18:11:52 modelPT:194] Please call the ModelPT.setup_test_data() or ModelPT.setup_multiple_test_data() method and provide a valid configuration file to setup the test data loader(s).\n",
      "    Test config : \n",
      "    manifest_filepath: null\n",
      "    sample_rate: 16000\n",
      "    batch_size: 16\n",
      "    shuffle: false\n",
      "    num_workers: 8\n",
      "    pin_memory: true\n",
      "    use_start_end_token: false\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-05-07 18:11:52 features:305] PADDING: 0\n",
      "[NeMo I 2025-05-07 18:11:53 rnnt_models:226] Using RNNT Loss : warprnnt_numba\n",
      "    Loss warprnnt_numba_kwargs: {'fastemit_lambda': 0.0, 'clamp': -1.0}\n",
      "[NeMo I 2025-05-07 18:11:53 rnnt_models:226] Using RNNT Loss : warprnnt_numba\n",
      "    Loss warprnnt_numba_kwargs: {'fastemit_lambda': 0.0, 'clamp': -1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2025-05-07 18:11:53 rnnt_loop_labels_computer:290] No conditional node support for Cuda.\n",
      "    Cuda graphs with while loops are disabled, decoding speed will be slower\n",
      "    Reason: Driver supports cuda toolkit version 12.2, but the driver needs to support at least 12,3. Please update your cuda driver.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-05-07 18:11:53 rnnt_models:226] Using RNNT Loss : warprnnt_numba\n",
      "    Loss warprnnt_numba_kwargs: {'fastemit_lambda': 0.0, 'clamp': -1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2025-05-07 18:11:53 rnnt_loop_labels_computer:290] No conditional node support for Cuda.\n",
      "    Cuda graphs with while loops are disabled, decoding speed will be slower\n",
      "    Reason: Driver supports cuda toolkit version 12.2, but the driver needs to support at least 12,3. Please update your cuda driver.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-05-07 18:11:54 save_restore_connector:275] Model EncDecHybridRNNTCTCBPEModel was successfully restored from /home/antpc/.cache/torch/NeMo/NeMo_2.4.0rc0/stt_en_fastconformer_hybrid_large_pc/465b32000fc320f5905fda11a1866ef6/stt_en_fastconformer_hybrid_large_pc.nemo.\n",
      "[NeMo I 2025-05-07 18:11:54 hybrid_rnnt_ctc_bpe_models:487] No `decoding_cfg` passed when changing decoding strategy, using internal config\n",
      "[NeMo I 2025-05-07 18:11:54 hybrid_rnnt_ctc_bpe_models:512] Changed decoding strategy of the CTC decoder to \n",
      "    strategy: greedy\n",
      "    preserve_alignments: null\n",
      "    compute_timestamps: null\n",
      "    word_seperator: ' '\n",
      "    segment_seperators:\n",
      "    - .\n",
      "    - '!'\n",
      "    - '?'\n",
      "    segment_gap_threshold: null\n",
      "    ctc_timestamp_type: all\n",
      "    batch_dim_index: 0\n",
      "    greedy:\n",
      "      preserve_alignments: false\n",
      "      compute_timestamps: false\n",
      "      preserve_frame_confidence: false\n",
      "      confidence_method_cfg:\n",
      "        name: entropy\n",
      "        entropy_type: tsallis\n",
      "        alpha: 0.33\n",
      "        entropy_norm: exp\n",
      "        temperature: DEPRECATED\n",
      "    beam:\n",
      "      beam_size: 4\n",
      "      search_type: default\n",
      "      preserve_alignments: false\n",
      "      compute_timestamps: false\n",
      "      return_best_hypothesis: true\n",
      "      beam_alpha: 1.0\n",
      "      beam_beta: 0.0\n",
      "      kenlm_path: null\n",
      "      flashlight_cfg:\n",
      "        lexicon_path: null\n",
      "        boost_path: null\n",
      "        beam_size_token: 16\n",
      "        beam_threshold: 20.0\n",
      "        unk_weight: -.inf\n",
      "        sil_weight: 0.0\n",
      "      pyctcdecode_cfg:\n",
      "        beam_prune_logp: -10.0\n",
      "        token_min_logp: -5.0\n",
      "        prune_history: false\n",
      "        hotwords: null\n",
      "        hotword_weight: 10.0\n",
      "    wfst:\n",
      "      beam_size: 4\n",
      "      search_type: riva\n",
      "      return_best_hypothesis: true\n",
      "      preserve_alignments: false\n",
      "      compute_timestamps: false\n",
      "      decoding_mode: nbest\n",
      "      open_vocabulary_decoding: false\n",
      "      beam_width: 10.0\n",
      "      lm_weight: 1.0\n",
      "      device: cuda\n",
      "      arpa_lm_path: null\n",
      "      wfst_lm_path: null\n",
      "      riva_decoding_cfg: {}\n",
      "      k2_decoding_cfg:\n",
      "        search_beam: 20.0\n",
      "        output_beam: 10.0\n",
      "        min_active_states: 30\n",
      "        max_active_states: 10000\n",
      "    confidence_cfg:\n",
      "      preserve_frame_confidence: false\n",
      "      preserve_token_confidence: false\n",
      "      preserve_word_confidence: false\n",
      "      exclude_blank: true\n",
      "      aggregation: min\n",
      "      tdt_include_duration: false\n",
      "      method_cfg:\n",
      "        name: entropy\n",
      "        entropy_type: tsallis\n",
      "        alpha: 0.33\n",
      "        entropy_norm: exp\n",
      "        temperature: DEPRECATED\n",
      "    temperature: 1.0\n",
      "    \n",
      "[NeMo I 2025-05-07 18:11:54 align:250] Flag use_local_attention is set to True => will try to use local attention for model if it allows it\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Transcribing:   0%|          | 0/1 [00:00<?, ?it/s][NeMo W 2025-05-07 18:13:39 ctc_greedy_decoding:168] CTC decoding strategy 'greedy' is slower than 'greedy_batch', which implements the same exact interface. Consider changing your strategy to 'greedy_batch' for a free performance improvement.\n",
      "Transcribing: 100%|██████████| 1/1 [01:44<00:00, 104.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-05-07 18:13:40 data_prep:828] Calculated that the model downsample factor is 8 and therefore the ASR model output timestep duration is 0.08 -- will use this for all batches\n",
      "Running alignment for: /hdd2/Aman/scripts/ML_assign/Dataset/manifest/case5_2023_05_10_manifest.json\n",
      "[NeMo I 2025-05-07 18:17:07 align:159] Hydra config: pretrained_name: stt_en_fastconformer_hybrid_large_pc\n",
      "    model_path: null\n",
      "    manifest_filepath: /hdd2/Aman/scripts/ML_assign/Dataset/manifest/case5_2023_05_10_manifest.json\n",
      "    output_dir: /hdd2/Aman/scripts/ML_assign/src/nfa_output/case5_2023_05_10_manifest\n",
      "    align_using_pred_text: false\n",
      "    transcribe_device: cpu\n",
      "    viterbi_device: cpu\n",
      "    batch_size: 1\n",
      "    use_local_attention: true\n",
      "    additional_segment_grouping_separator: '|'\n",
      "    audio_filepath_parts_in_utt_id: 1\n",
      "    use_buffered_chunked_streaming: false\n",
      "    chunk_len_in_secs: 1.6\n",
      "    total_buffer_in_secs: 4.0\n",
      "    chunk_batch_size: 32\n",
      "    simulate_cache_aware_streaming: false\n",
      "    save_output_file_formats:\n",
      "    - ctm\n",
      "    - ass\n",
      "    ctm_file_config:\n",
      "      remove_blank_tokens: false\n",
      "      minimum_timestamp_duration: 0.0\n",
      "    ass_file_config:\n",
      "      fontsize: 20\n",
      "      vertical_alignment: center\n",
      "      resegment_text_to_fill_space: false\n",
      "      max_lines_per_segment: 2\n",
      "      text_already_spoken_rgb:\n",
      "      - 49\n",
      "      - 46\n",
      "      - 61\n",
      "      text_being_spoken_rgb:\n",
      "      - 57\n",
      "      - 171\n",
      "      - 9\n",
      "      text_not_yet_spoken_rgb:\n",
      "      - 194\n",
      "      - 193\n",
      "      - 199\n",
      "    \n",
      "[NeMo I 2025-05-07 18:17:07 align:228] Device to be used for transcription step (`transcribe_device`) is cpu\n",
      "[NeMo I 2025-05-07 18:17:07 align:234] Device to be used for viterbi step (`viterbi_device`) is cpu\n",
      "[NeMo I 2025-05-07 18:17:07 cloud:58] Found existing object /home/antpc/.cache/torch/NeMo/NeMo_2.4.0rc0/stt_en_fastconformer_hybrid_large_pc/465b32000fc320f5905fda11a1866ef6/stt_en_fastconformer_hybrid_large_pc.nemo.\n",
      "[NeMo I 2025-05-07 18:17:07 cloud:64] Re-using file from: /home/antpc/.cache/torch/NeMo/NeMo_2.4.0rc0/stt_en_fastconformer_hybrid_large_pc/465b32000fc320f5905fda11a1866ef6/stt_en_fastconformer_hybrid_large_pc.nemo\n",
      "[NeMo I 2025-05-07 18:17:07 common:827] Instantiating model from pre-trained checkpoint\n",
      "[NeMo I 2025-05-07 18:17:09 mixins:181] Tokenizer SentencePieceTokenizer initialized with 1024 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2025-05-07 18:17:10 modelPT:180] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
      "    Train config : \n",
      "    is_concat: false\n",
      "    concat_sampling_technique: random\n",
      "    concat_sampling_temperature: 8\n",
      "    concat_shuffle: true\n",
      "    concat_sampling_scale: 1.0\n",
      "    manifest_filepath:\n",
      "    - - /data/mml/en/tarred_train/pcstrip_sharded_manifests/manifest__OP_0..511_CL_.json\n",
      "    tarred_audio_filepaths:\n",
      "    - - /data/mml/en/tarred_train/audio__OP_0..511_CL_.tar\n",
      "    concat_sampling_probabilities:\n",
      "    - 1.0\n",
      "    sample_rate: 16000\n",
      "    batch_size: 16\n",
      "    shuffle: true\n",
      "    num_workers: 8\n",
      "    pin_memory: true\n",
      "    use_start_end_token: false\n",
      "    trim_silence: false\n",
      "    max_duration: 20\n",
      "    min_duration: 0.1\n",
      "    is_tarred: true\n",
      "    shard_manifests: true\n",
      "    defer_setup: true\n",
      "    shuffle_n: 528387\n",
      "    bucketing_strategy: synced_randomized\n",
      "    bucketing_batch_size: null\n",
      "    \n",
      "[NeMo W 2025-05-07 18:17:10 modelPT:187] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
      "    Validation config : \n",
      "    ds_item:\n",
      "    - name: en_\n",
      "      is_concat: true\n",
      "      concat_sampling_technique: random\n",
      "      concat_sampling_seed: 1234\n",
      "      concat_shuffle: false\n",
      "      concat_sampling_scale: 0.25\n",
      "      concat_sampling_probabilities:\n",
      "      - 0.088929287\n",
      "      - 0.116404906\n",
      "      - 0.101879436\n",
      "      - 0.171855007\n",
      "      - 0.160962192\n",
      "      - 0.092674082\n",
      "      - 0.041152255\n",
      "      - 0.226142835\n",
      "      manifest_filepath:\n",
      "      - - /data/mml/en/val_test/fisher/audio_manifest_dev_clean_pcstrip_en.json\n",
      "        - /data/mml/en/val_test/europarl/audio_manifest_dev_clean_pcstrip_en.json\n",
      "        - /data/mml/en/val_test/librispeech/audio_manifest_dev_clean_pcstrip_en.json\n",
      "        - /data/mml/en/val_test/mcv11/audio_manifest_dev_clean_pcstrip_en.json\n",
      "        - /data/mml/en/val_test/nsc1/audio_manifest_dev_clean_pcstrip_en.json\n",
      "        - /data/mml/en/val_test/SPGI/audio_manifest_dev_clean_pcstrip_en.json\n",
      "        - /data/mml/en/val_test/voxpopuli/audio_manifest_dev_clean_pcstrip_en.json\n",
      "        - /data/mml/en/val_test/mls_dev/mls_dev_pcstrip_en.json\n",
      "      sample_rate: 16000\n",
      "      batch_size: 64\n",
      "      shuffle: false\n",
      "      num_workers: 1\n",
      "      pin_memory: true\n",
      "      max_duration: 20.0\n",
      "      min_duration: 0.1\n",
      "      use_start_end_token: false\n",
      "    - name: fisher_\n",
      "      is_concat: false\n",
      "      manifest_filepath: /data/mml/en/val_test/fisher/audio_manifest_dev_clean_pcstrip_en.json\n",
      "      sample_rate: 16000\n",
      "      batch_size: 64\n",
      "      shuffle: false\n",
      "      num_workers: 1\n",
      "      pin_memory: true\n",
      "      max_duration: 20.0\n",
      "      min_duration: 0.1\n",
      "      use_start_end_token: false\n",
      "    - name: europarl_\n",
      "      is_concat: false\n",
      "      manifest_filepath: /data/mml/en/val_test/europarl/audio_manifest_dev_clean_pcstrip_en.json\n",
      "      sample_rate: 16000\n",
      "      batch_size: 64\n",
      "      shuffle: false\n",
      "      num_workers: 1\n",
      "      pin_memory: true\n",
      "      max_duration: 20.0\n",
      "      min_duration: 0.1\n",
      "      use_start_end_token: false\n",
      "    - name: librispeech_\n",
      "      is_concat: false\n",
      "      manifest_filepath: /data/mml/en/val_test/librispeech/audio_manifest_dev_clean_pcstrip_en.json\n",
      "      sample_rate: 16000\n",
      "      batch_size: 64\n",
      "      shuffle: false\n",
      "      num_workers: 1\n",
      "      pin_memory: true\n",
      "      max_duration: 20.0\n",
      "      min_duration: 0.1\n",
      "      use_start_end_token: false\n",
      "    - name: mcv11_\n",
      "      is_concat: false\n",
      "      manifest_filepath: /data/mml/en/val_test/mcv11/audio_manifest_dev_clean_pcstrip_en.json\n",
      "      sample_rate: 16000\n",
      "      batch_size: 64\n",
      "      shuffle: false\n",
      "      num_workers: 1\n",
      "      pin_memory: true\n",
      "      max_duration: 20.0\n",
      "      min_duration: 0.1\n",
      "      use_start_end_token: false\n",
      "    - name: nsc1_\n",
      "      is_concat: false\n",
      "      manifest_filepath: /data/mml/en/val_test/nsc1/audio_manifest_dev_clean_pcstrip_en.json\n",
      "      sample_rate: 16000\n",
      "      batch_size: 64\n",
      "      shuffle: false\n",
      "      num_workers: 1\n",
      "      pin_memory: true\n",
      "      max_duration: 20.0\n",
      "      min_duration: 0.1\n",
      "      use_start_end_token: false\n",
      "    - name: spgi_\n",
      "      is_concat: false\n",
      "      manifest_filepath: /data/mml/en/val_test/SPGI/audio_manifest_dev_clean_pcstrip_en.json\n",
      "      sample_rate: 16000\n",
      "      batch_size: 64\n",
      "      shuffle: false\n",
      "      num_workers: 1\n",
      "      pin_memory: true\n",
      "      max_duration: 20.0\n",
      "      min_duration: 0.1\n",
      "      use_start_end_token: false\n",
      "    - name: voxpopuli_\n",
      "      is_concat: false\n",
      "      manifest_filepath: /data/mml/en/val_test/voxpopuli/audio_manifest_dev_clean_pcstrip_en.json\n",
      "      sample_rate: 16000\n",
      "      batch_size: 64\n",
      "      shuffle: false\n",
      "      num_workers: 1\n",
      "      pin_memory: true\n",
      "      max_duration: 20.0\n",
      "      min_duration: 0.1\n",
      "      use_start_end_token: false\n",
      "    - name: mls_\n",
      "      is_concat: false\n",
      "      manifest_filepath: /data/mml/en/val_test/mls_dev/mls_dev_pcstrip_en.json\n",
      "      sample_rate: 16000\n",
      "      batch_size: 64\n",
      "      shuffle: false\n",
      "      num_workers: 1\n",
      "      pin_memory: true\n",
      "      max_duration: 20.0\n",
      "      min_duration: 0.1\n",
      "      use_start_end_token: false\n",
      "    max_duration: 20\n",
      "    \n",
      "[NeMo W 2025-05-07 18:17:10 modelPT:194] Please call the ModelPT.setup_test_data() or ModelPT.setup_multiple_test_data() method and provide a valid configuration file to setup the test data loader(s).\n",
      "    Test config : \n",
      "    manifest_filepath: null\n",
      "    sample_rate: 16000\n",
      "    batch_size: 16\n",
      "    shuffle: false\n",
      "    num_workers: 8\n",
      "    pin_memory: true\n",
      "    use_start_end_token: false\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-05-07 18:17:10 features:305] PADDING: 0\n",
      "[NeMo I 2025-05-07 18:17:11 rnnt_models:226] Using RNNT Loss : warprnnt_numba\n",
      "    Loss warprnnt_numba_kwargs: {'fastemit_lambda': 0.0, 'clamp': -1.0}\n",
      "[NeMo I 2025-05-07 18:17:11 rnnt_models:226] Using RNNT Loss : warprnnt_numba\n",
      "    Loss warprnnt_numba_kwargs: {'fastemit_lambda': 0.0, 'clamp': -1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2025-05-07 18:17:11 rnnt_loop_labels_computer:290] No conditional node support for Cuda.\n",
      "    Cuda graphs with while loops are disabled, decoding speed will be slower\n",
      "    Reason: Driver supports cuda toolkit version 12.2, but the driver needs to support at least 12,3. Please update your cuda driver.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-05-07 18:17:12 rnnt_models:226] Using RNNT Loss : warprnnt_numba\n",
      "    Loss warprnnt_numba_kwargs: {'fastemit_lambda': 0.0, 'clamp': -1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2025-05-07 18:17:12 rnnt_loop_labels_computer:290] No conditional node support for Cuda.\n",
      "    Cuda graphs with while loops are disabled, decoding speed will be slower\n",
      "    Reason: Driver supports cuda toolkit version 12.2, but the driver needs to support at least 12,3. Please update your cuda driver.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-05-07 18:17:12 save_restore_connector:275] Model EncDecHybridRNNTCTCBPEModel was successfully restored from /home/antpc/.cache/torch/NeMo/NeMo_2.4.0rc0/stt_en_fastconformer_hybrid_large_pc/465b32000fc320f5905fda11a1866ef6/stt_en_fastconformer_hybrid_large_pc.nemo.\n",
      "[NeMo I 2025-05-07 18:17:12 hybrid_rnnt_ctc_bpe_models:487] No `decoding_cfg` passed when changing decoding strategy, using internal config\n",
      "[NeMo I 2025-05-07 18:17:12 hybrid_rnnt_ctc_bpe_models:512] Changed decoding strategy of the CTC decoder to \n",
      "    strategy: greedy\n",
      "    preserve_alignments: null\n",
      "    compute_timestamps: null\n",
      "    word_seperator: ' '\n",
      "    segment_seperators:\n",
      "    - .\n",
      "    - '!'\n",
      "    - '?'\n",
      "    segment_gap_threshold: null\n",
      "    ctc_timestamp_type: all\n",
      "    batch_dim_index: 0\n",
      "    greedy:\n",
      "      preserve_alignments: false\n",
      "      compute_timestamps: false\n",
      "      preserve_frame_confidence: false\n",
      "      confidence_method_cfg:\n",
      "        name: entropy\n",
      "        entropy_type: tsallis\n",
      "        alpha: 0.33\n",
      "        entropy_norm: exp\n",
      "        temperature: DEPRECATED\n",
      "    beam:\n",
      "      beam_size: 4\n",
      "      search_type: default\n",
      "      preserve_alignments: false\n",
      "      compute_timestamps: false\n",
      "      return_best_hypothesis: true\n",
      "      beam_alpha: 1.0\n",
      "      beam_beta: 0.0\n",
      "      kenlm_path: null\n",
      "      flashlight_cfg:\n",
      "        lexicon_path: null\n",
      "        boost_path: null\n",
      "        beam_size_token: 16\n",
      "        beam_threshold: 20.0\n",
      "        unk_weight: -.inf\n",
      "        sil_weight: 0.0\n",
      "      pyctcdecode_cfg:\n",
      "        beam_prune_logp: -10.0\n",
      "        token_min_logp: -5.0\n",
      "        prune_history: false\n",
      "        hotwords: null\n",
      "        hotword_weight: 10.0\n",
      "    wfst:\n",
      "      beam_size: 4\n",
      "      search_type: riva\n",
      "      return_best_hypothesis: true\n",
      "      preserve_alignments: false\n",
      "      compute_timestamps: false\n",
      "      decoding_mode: nbest\n",
      "      open_vocabulary_decoding: false\n",
      "      beam_width: 10.0\n",
      "      lm_weight: 1.0\n",
      "      device: cuda\n",
      "      arpa_lm_path: null\n",
      "      wfst_lm_path: null\n",
      "      riva_decoding_cfg: {}\n",
      "      k2_decoding_cfg:\n",
      "        search_beam: 20.0\n",
      "        output_beam: 10.0\n",
      "        min_active_states: 30\n",
      "        max_active_states: 10000\n",
      "    confidence_cfg:\n",
      "      preserve_frame_confidence: false\n",
      "      preserve_token_confidence: false\n",
      "      preserve_word_confidence: false\n",
      "      exclude_blank: true\n",
      "      aggregation: min\n",
      "      tdt_include_duration: false\n",
      "      method_cfg:\n",
      "        name: entropy\n",
      "        entropy_type: tsallis\n",
      "        alpha: 0.33\n",
      "        entropy_norm: exp\n",
      "        temperature: DEPRECATED\n",
      "    temperature: 1.0\n",
      "    \n",
      "[NeMo I 2025-05-07 18:17:12 align:250] Flag use_local_attention is set to True => will try to use local attention for model if it allows it\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Transcribing:   0%|          | 0/1 [00:00<?, ?it/s][NeMo W 2025-05-07 18:18:28 ctc_greedy_decoding:168] CTC decoding strategy 'greedy' is slower than 'greedy_batch', which implements the same exact interface. Consider changing your strategy to 'greedy_batch' for a free performance improvement.\n",
      "Transcribing: 100%|██████████| 1/1 [01:16<00:00, 76.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-05-07 18:18:29 data_prep:828] Calculated that the model downsample factor is 8 and therefore the ASR model output timestep duration is 0.08 -- will use this for all batches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/hdd2/Aman/scripts/ML_assign/src/NeMo/tools/nemo_forced_aligner/align.py\", line 352, in <module>\n",
      "    main()\n",
      "  File \"/hdd2/Aman/envs/semascore/lib/python3.10/site-packages/nemo/core/config/hydra_runner.py\", line 129, in wrapper\n",
      "    _run_hydra(\n",
      "  File \"/hdd2/Aman/envs/semascore/lib/python3.10/site-packages/hydra/_internal/utils.py\", line 394, in _run_hydra\n",
      "    _run_app(\n",
      "  File \"/hdd2/Aman/envs/semascore/lib/python3.10/site-packages/hydra/_internal/utils.py\", line 457, in _run_app\n",
      "    run_and_report(\n",
      "  File \"/hdd2/Aman/envs/semascore/lib/python3.10/site-packages/hydra/_internal/utils.py\", line 220, in run_and_report\n",
      "    return func()\n",
      "  File \"/hdd2/Aman/envs/semascore/lib/python3.10/site-packages/hydra/_internal/utils.py\", line 458, in <lambda>\n",
      "    lambda: hydra.run(\n",
      "  File \"/hdd2/Aman/envs/semascore/lib/python3.10/site-packages/hydra/_internal/hydra.py\", line 119, in run\n",
      "    ret = run_job(\n",
      "  File \"/hdd2/Aman/envs/semascore/lib/python3.10/site-packages/hydra/core/utils.py\", line 186, in run_job\n",
      "    ret.return_value = task_function(task_cfg)\n",
      "  File \"/hdd2/Aman/scripts/ML_assign/src/NeMo/tools/nemo_forced_aligner/align.py\", line 330, in main\n",
      "    alignments_batch = viterbi_decoding(log_probs_batch, y_batch, T_batch, U_batch, viterbi_device)\n",
      "  File \"/hdd2/Aman/scripts/ML_assign/src/NeMo/tools/nemo_forced_aligner/utils/viterbi_decoding.py\", line 76, in viterbi_decoding\n",
      "    e_current = torch.gather(input=log_probs_padded[:, t, :], dim=1, index=y_batch)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 31\u001b[0m\n\u001b[1;32m     16\u001b[0m command \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpython\u001b[39m\u001b[38;5;124m\"\u001b[39m, align_script,\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpretrained_name=stt_en_fastconformer_hybrid_large_pc\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mass_file_config.text_not_yet_spoken_rgb=[194,193,199]\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     28\u001b[0m ]\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRunning alignment for: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmanifest\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 31\u001b[0m \u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/subprocess.py:505\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    503\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Popen(\u001b[38;5;241m*\u001b[39mpopenargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;28;01mas\u001b[39;00m process:\n\u001b[1;32m    504\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 505\u001b[0m         stdout, stderr \u001b[38;5;241m=\u001b[39m \u001b[43mprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcommunicate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    506\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m TimeoutExpired \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    507\u001b[0m         process\u001b[38;5;241m.\u001b[39mkill()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/subprocess.py:1146\u001b[0m, in \u001b[0;36mPopen.communicate\u001b[0;34m(self, input, timeout)\u001b[0m\n\u001b[1;32m   1144\u001b[0m         stderr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m   1145\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m-> 1146\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1148\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/subprocess.py:1209\u001b[0m, in \u001b[0;36mPopen.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1207\u001b[0m     endtime \u001b[38;5;241m=\u001b[39m _time() \u001b[38;5;241m+\u001b[39m timeout\n\u001b[1;32m   1208\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1209\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1210\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1211\u001b[0m     \u001b[38;5;66;03m# https://bugs.python.org/issue25942\u001b[39;00m\n\u001b[1;32m   1212\u001b[0m     \u001b[38;5;66;03m# The first keyboard interrupt waits briefly for the child to\u001b[39;00m\n\u001b[1;32m   1213\u001b[0m     \u001b[38;5;66;03m# exit under the common assumption that it also received the ^C\u001b[39;00m\n\u001b[1;32m   1214\u001b[0m     \u001b[38;5;66;03m# generated SIGINT and will exit rapidly.\u001b[39;00m\n\u001b[1;32m   1215\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/subprocess.py:1959\u001b[0m, in \u001b[0;36mPopen._wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1957\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1958\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m  \u001b[38;5;66;03m# Another thread waited.\u001b[39;00m\n\u001b[0;32m-> 1959\u001b[0m (pid, sts) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1960\u001b[0m \u001b[38;5;66;03m# Check the pid and loop as waitpid has been known to\u001b[39;00m\n\u001b[1;32m   1961\u001b[0m \u001b[38;5;66;03m# return 0 even without WNOHANG in odd situations.\u001b[39;00m\n\u001b[1;32m   1962\u001b[0m \u001b[38;5;66;03m# http://bugs.python.org/issue14396.\u001b[39;00m\n\u001b[1;32m   1963\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pid \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpid:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/subprocess.py:1917\u001b[0m, in \u001b[0;36mPopen._try_wait\u001b[0;34m(self, wait_flags)\u001b[0m\n\u001b[1;32m   1915\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"All callers to this function MUST hold self._waitpid_lock.\"\"\"\u001b[39;00m\n\u001b[1;32m   1916\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1917\u001b[0m     (pid, sts) \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwaitpid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwait_flags\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1918\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mChildProcessError\u001b[39;00m:\n\u001b[1;32m   1919\u001b[0m     \u001b[38;5;66;03m# This happens if SIGCLD is set to be ignored or waiting\u001b[39;00m\n\u001b[1;32m   1920\u001b[0m     \u001b[38;5;66;03m# for child processes has otherwise been disabled for our\u001b[39;00m\n\u001b[1;32m   1921\u001b[0m     \u001b[38;5;66;03m# process.  This child is dead, we can't get the status.\u001b[39;00m\n\u001b[1;32m   1922\u001b[0m     pid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpid\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "manifest_dir = \"/hdd2/Aman/scripts/ML_assign/Dataset/manifest\"\n",
    "align_script = \"/hdd2/Aman/scripts/ML_assign/src/NeMo/tools/nemo_forced_aligner/align.py\"\n",
    "output_base = \"/hdd2/Aman/scripts/ML_assign/src/nfa_output\"\n",
    "\n",
    "# Find all manifest files\n",
    "manifest_files = glob.glob(os.path.join(manifest_dir, \"*.json\"))\n",
    "\n",
    "# Run the aligner for each manifest\n",
    "for manifest in manifest_files:\n",
    "    output_dir = os.path.join(output_base, os.path.basename(manifest).replace(\".json\", \"\"))\n",
    "    \n",
    "    command = [\n",
    "        \"python\", align_script,\n",
    "        f'pretrained_name=stt_en_fastconformer_hybrid_large_pc',\n",
    "        f'manifest_filepath={manifest}',\n",
    "        f'output_dir={output_dir}',\n",
    "        f'additional_segment_grouping_separator=|',\n",
    "        f'transcribe_device=cpu',\n",
    "        f'viterbi_device=cpu'\n",
    "    ]\n",
    "    \n",
    "    print(f\"Running alignment for: {manifest}\")\n",
    "    subprocess.run(command, check=True)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "semascore",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
